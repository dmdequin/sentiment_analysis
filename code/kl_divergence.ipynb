{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d11d6da",
   "metadata": {},
   "source": [
    "This notebook is used to:\n",
    "* write and debug the script for computing kl-divergence\n",
    "* visualize word frequency differences between domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6352bc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b10fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144a036",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_loader(PATH):\n",
    "    text = pd.read_csv(PATH, names=['review','sentiment']) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb5b57",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d182843",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_1 = 'music_dev'  # name of interim csv file. For example: # games_train\n",
    "FILE_2 = 'games_val'  # name of comparison interim csv file. For example: # sew_train\n",
    "FILE_NAME = 'games'\n",
    "N_DIS = 100   # number of dissimilar embeddings to select\n",
    "# python3 cosine.py 'music_dev' 'sew_val' 'sew' 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6b5b4",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Interim CSV file and split into X and y\n",
    "data_1 = csv_loader('../data/interim/' + FILE_1 + '.csv')\n",
    "X_1, y_1 = data_1[['review']], data_1[['sentiment']]\n",
    "X_1 = X_1[0:1000]\n",
    "\n",
    "# Load Interim CSV file and split into X and y\n",
    "data_2 = csv_loader('../data/interim/' + FILE_2 + '.csv')\n",
    "X_2, y_2 = data_2[['review']], data_2[['sentiment']]\n",
    "X_2 = X_2[0:1000]\n",
    "len(X_1), len(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a31d259",
   "metadata": {},
   "source": [
    "# Tokenize, Remove Stop words, Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e884da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))+list(string.punctuation)\n",
    "#punct = list(string.punctuation)\n",
    "\n",
    "# Tokenize each review and lowercase everything\n",
    "corp_1 = []\n",
    "for i in range(len(X_1)): \n",
    "    row = X_1.iloc[i]['review']\n",
    "    token_review = word_tokenize(row)\n",
    "    filtered = [corp_1.append(w.lower()) for w in token_review if not w.lower() in stop_words]\n",
    "\n",
    "# List of all Words    \n",
    "all_words = list(set(corp_1))\n",
    "\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_2 = []\n",
    "for i in range(1,len(X_2)): \n",
    "    row = X_2.iloc[i]['review']\n",
    "    token_review = word_tokenize(row)\n",
    "    filtered = [corp_2.append(w.lower()) for w in token_review if not w.lower() in stop_words]\n",
    "\n",
    "# Add words from corp 2 to list\n",
    "all_words = all_words + corp_2\n",
    "all_words = list(set(all_words))\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d45f2c",
   "metadata": {},
   "source": [
    "# Word Count Dictionaries for Corpses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of word to index, and index to word\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "\n",
    "# size of corpses\n",
    "count_corp1 = len(corp_1)\n",
    "count_corp2 = len(corp_2)\n",
    "\n",
    "# Initialize Dictionaries for both corpus\n",
    "dict_1 = {}\n",
    "for i in range(len(all_words)):\n",
    "    word_to_idx[all_words[i]] = i\n",
    "    idx_to_word[i] = all_words[i]\n",
    "    dict_1[word_to_idx[all_words[i]]] = 0.01 # add 0.01 as starter, to avoid 0 division\n",
    "    \n",
    "dict_2 = {}\n",
    "for i in all_words:\n",
    "    dict_2[word_to_idx[i]] = 0.01\n",
    "\n",
    "# get word count for corpus 1\n",
    "for i in corp_1:\n",
    "    dict_1[word_to_idx[i]] += 1\n",
    "\n",
    "# get word count for corpus 2\n",
    "for i in corp_2:\n",
    "    dict_2[word_to_idx[i]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d5502",
   "metadata": {},
   "source": [
    "# Convert to Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df2063",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get word probabilites for corp 1\n",
    "corp1_prob = {}\n",
    "for k,v in dict_1.items():\n",
    "    prob = v/count_corp1    \n",
    "    corp1_prob[k] = prob\n",
    "prob1_list = list(corp1_prob.values())\n",
    "\n",
    "# get word probabilites for corp 2\n",
    "corp2_prob = {}\n",
    "for k,v in dict_2.items():\n",
    "    prob2 = v/count_corp2\n",
    "    corp2_prob[k] = prob2\n",
    "prob2_list = list(corp2_prob.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(a, b):\n",
    "    sums = 0\n",
    "    for i in range(len(a)):\n",
    "        if b[i]!=0.01:\n",
    "            sums += a[i] * np.log(a[i]/b[i])\n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844809be",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence(prob1_list, prob2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900ebed",
   "metadata": {},
   "source": [
    "# Plot probabilities and frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = max(dict_2.values())  # maximum value\n",
    "max_keys = [k for k, v in dict_2.items() if v == max_value]\n",
    "idx_to_word[max_keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada45513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word probabilities\n",
    "sns.lineplot(x=list(corp1_prob.keys()),y=corp1_prob.values())\n",
    "sns.lineplot(x=list(corp2_prob.keys()),y=corp2_prob.values(), alpha=.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequencies\n",
    "sns.lineplot(x=list(dict_1.keys()),y=dict_1.values())\n",
    "sns.lineplot(x=list(dict_2.keys()),y=dict_2.values(), alpha=.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4a6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
