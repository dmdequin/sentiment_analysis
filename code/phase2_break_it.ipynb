{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90e347c",
   "metadata": {},
   "source": [
    "# Phase 2: Break It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b513ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# install checklist\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "# install spacy\n",
    "import spacy\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "742db0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/interim/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f2b8b399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        positive\n",
       "1        negative\n",
       "2        negative\n",
       "3        positive\n",
       "4        positive\n",
       "           ...   \n",
       "99995    positive\n",
       "99996    negative\n",
       "99997    positive\n",
       "99998    negative\n",
       "99999    negative\n",
       "Name: sentiment, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_json('../data/raw/music_reviews_train.json.gz', lines=True)\n",
    "data_train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7a7dbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6d81c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "label = [] \n",
    "c = 0\n",
    "n = 100 # set how many training examples to use.\n",
    "with open(TRAIN, mode='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    for lines in csvFile:\n",
    "        if c < n:\n",
    "            input_data.append(lines[0])\n",
    "            label.append(int(lines[1]))\n",
    "            c+=1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7714ffa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tkn(sentence):\n",
    "    \"\"\"Function to find all tokens in a given sentence\n",
    "    \"\"\"\n",
    "    tok = re.compile('[\\'\\\"]|[A-Za-z]+|[.?!:\\'\\\"]+')\n",
    "    \n",
    "    return tok.findall(sentence)\n",
    "    \n",
    "def splitsies(para):\n",
    "    punct = re.compile('[.?!:]')\n",
    "    t = punct.split(para)\n",
    "    spl= []\n",
    "    for i in t:\n",
    "        temp = tkn(i)\n",
    "        if len(temp) > 0:\n",
    "            spl.append(temp)\n",
    "        \n",
    "    return spl\n",
    "    \n",
    "#splitsies(input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ae455c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!\n",
      "Label: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Sentence 1:\\n{input_data[0]}\\nLabel: {label[0]}')\n",
    "type(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5ecf3c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dmdequin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gotta listen to this!',\n",
       " 'So creative!',\n",
       " 'Love his music - the words, the message!',\n",
       " 'Some of my favorite songs on this CD.',\n",
       " 'I should have bought it years ago!']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "token_text = []\n",
    "for para in input_data:\n",
    "    token_text.append(sent_tokenize(para))\n",
    "token_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "75b118f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f459c66",
   "metadata": {},
   "source": [
    "# Add Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0b6da5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!',\n",
       " 'Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I shoul dhave bought it years ago!']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = Perturb.perturb(input_data, Perturb.add_typos)\n",
    "ret.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b79694d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I shoul dhave bought it years ago!'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This just typos one sentence in input paragraph\n",
    "typo = []\n",
    "for thing in ret.data:\n",
    "    typo.append(thing[1])\n",
    "typo[0]\n",
    "#typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "cc9a0064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Five Stars I love all of his music!', '!..']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If \"sentence\" in para is only 1 char, add \"..\" to avoid errors\n",
    "for i in range(len(token_text)):\n",
    "    for j in range(len(token_text[i])):\n",
    "        if len(token_text[i][j]) <= 1:\n",
    "            token_text[i][j] = token_text[i][j]+'..'\n",
    "            \n",
    "token_text[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7b18f2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Five Stars I love all ofh is music!', '!..']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typo every sentence in every paragraph\n",
    "typoed = []\n",
    "lala = []\n",
    "for i in range(len(token_text)):\n",
    "    ret = Perturb.perturb(token_text[i], Perturb.add_typos)\n",
    "    typos = []\n",
    "    for sent in ret.data:\n",
    "        typos.append(sent[1])\n",
    "        lala.append(sent[1])\n",
    "    typoed.append(typos)\n",
    "typoed[6]\n",
    "#typoed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1f40fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to list of paragraphs WOWWWW\n",
    "tp = []\n",
    "for i in range(len(typoed)):\n",
    "    para = \"\"\n",
    "    for j in range(len(typoed[i])):\n",
    "        para = para + typoed[i][j] + \" \"\n",
    "    tp.append(para)\n",
    "#tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510097fa",
   "metadata": {},
   "source": [
    "# POS tag data\n",
    "\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "When you call nlp on a text, spaCy will tokenize it and then call each component on the Doc, in order. It then returns the processed Doc that you can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "60ffb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = list(nlp.pipe(tp))\n",
    "#for doc in nlp.pipe(input_data):\n",
    "    # Do something with the doc here\n",
    "#    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c540f9",
   "metadata": {},
   "source": [
    "# Remove End Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9aa950bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Gotta listent o this! So craetive! Love his msuic - the words, the message! Some of my favorite songs ont his CD. I sohuld have bought it years ago! ,\n",
       " 'Gotta listent o this! So craetive! Love his msuic - the words, the message! Some of my favorite songs ont his CD. I sohuld have bought it years ago')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata[0], Perturb.strip_punctuation(pdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "187e442c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gotta listent o this! So craetive! Love his msuic - the words, the message! Some of my favorite songs ont his CD. I sohuld have bought it years ago'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = Perturb.perturb(pdata, Perturb.punctuation)\n",
    "no_punct = []\n",
    "for i in ret.data:\n",
    "    no_punct.append(i[1])\n",
    "no_punct[0]\n",
    "len(no_punct)\n",
    "no_punct[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afab46f",
   "metadata": {},
   "source": [
    "# Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2b9f21ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gotta listent o this! So craetive! Love his msuic - the words, the message! Some of my favorite songs ont his CD. I sohuld have bought it years ago'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punct[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "02351295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gotta listent o this! So craetive! Love his msuic - the words, the message! Some of my favorite songs ont his CD. I sohuld have bought it years ago"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata = list(nlp.pipe(no_punct))\n",
    "pdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "44e7d554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This negates only 43 sentences\n",
    "nega = []\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    ret = Perturb.remove_negation(pdata[i])\n",
    "    if ret == None:  # if nothing changes\n",
    "        nega.append(pdata[i]) # append original sentence\n",
    "    else: \n",
    "        nega.append(ret) # append negated paragraph\n",
    "        count +=1\n",
    "len(nega)\n",
    "\n",
    "# Negation doesn't really change the sentiment labels, so they remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "77766f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "070bd972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# negate every sent in every para\\nneg = []\\nfor i in range(100):\\n    ppdata = []\\n    \\n    # break down into sentences\\n    sentenced = sent_tokenize(str(no_punct[i]))\\n    pdata = list(nlp.pipe(sentenced))\\n    para = []\\n    # for each sentence in para\\n    for sent in pdata:\\n        ret = Perturb.remove_negation(sent)\\n        if ret == None:  # if nothing changes\\n            para.append(sent) # append original sentence\\n        else:\\n            para.append(ret[1]) # append negated sentence\\n    ppdata.append(para)\\n    neg.append(ppdata)\\nneg'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# negate every sent in every para\n",
    "neg = []\n",
    "for i in range(100):\n",
    "    ppdata = []\n",
    "    \n",
    "    # break down into sentences\n",
    "    sentenced = sent_tokenize(str(no_punct[i]))\n",
    "    pdata = list(nlp.pipe(sentenced))\n",
    "    para = []\n",
    "    # for each sentence in para\n",
    "    for sent in pdata:\n",
    "        ret = Perturb.remove_negation(sent)\n",
    "        if ret == None:  # if nothing changes\n",
    "            para.append(sent) # append original sentence\n",
    "        else:\n",
    "            para.append(ret[1]) # append negated sentence\n",
    "    ppdata.append(para)\n",
    "    neg.append(ppdata)\n",
    "neg\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3cbdac51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ret = Perturb.perturb(pdata, Perturb.remove_negation)\\nnegated = []\\nfor i in ret.data:\\n    negated.append(i[1])\\nnegated[0]\\nlen(negated)\\npdata[0]'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"ret = Perturb.perturb(pdata, Perturb.remove_negation)\n",
    "negated = []\n",
    "for i in ret.data:\n",
    "    negated.append(i[1])\n",
    "negated[0]\n",
    "len(negated)\n",
    "pdata[0]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cc3de",
   "metadata": {},
   "source": [
    "# Make Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6a0b04b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reviewText': Gotta listent o this! So craetive! Love his msuic - the words, the message! Some of my favorite songs ont his CD. I sohuld have bought it years ago, 'sentiment': 'positive', 'category': \"['typos', 'punct', 'negation']\"}\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for i in range(len(input_data)):\n",
    "    dicti = {}\n",
    "    dicti['reviewText'] = nega[i]\n",
    "    dicti['sentiment'] = labels[i]\n",
    "    dicti['category'] = \"['typos', 'punct', 'negation']\"\n",
    "    output.append(dicti)\n",
    "len(output)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ae0ca1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/predictions/dee_dump.json', 'w') as fout:\n",
    "#    json.dump(output , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "dfcb8acf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Doc is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_379/3654863571.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'../data/predictions/dee_dump.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_379/3654863571.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'../data/predictions/dee_dump.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Doc is not JSON serializable"
     ]
    }
   ],
   "source": [
    "test_json = [json.dumps(i) for i in output]\n",
    "with open ('../data/predictions/dee_dump.json', 'w') as file:\n",
    "    file.writelines(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa07f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc4ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9c101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
