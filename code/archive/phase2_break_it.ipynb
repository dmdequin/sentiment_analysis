{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af3eff0",
   "metadata": {},
   "source": [
    "This notebook was used for phase 2 to create difficult cases for the submission requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e347c",
   "metadata": {},
   "source": [
    "# Phase 2: Break It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742db0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/interim/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_json('../data/raw/music_reviews_train.json.gz', lines=True)\n",
    "data_train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7dbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "label = [] \n",
    "c = 0\n",
    "n = 100 # set how many training examples to use.\n",
    "with open(TRAIN, mode='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    for lines in csvFile:\n",
    "        if c < n:\n",
    "            input_data.append(lines[0])\n",
    "            label.append(int(lines[1]))\n",
    "            c+=1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714ffa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tkn(sentence):\n",
    "    \"\"\"Function to find all tokens in a given sentence\n",
    "    \"\"\"\n",
    "    tok = re.compile('[\\'\\\"]|[A-Za-z]+|[.?!:\\'\\\"]+')\n",
    "    \n",
    "    return tok.findall(sentence)\n",
    "    \n",
    "def splitsies(para):\n",
    "    punct = re.compile('[.?!:]')\n",
    "    t = punct.split(para)\n",
    "    spl= []\n",
    "    for i in t:\n",
    "        temp = tkn(i)\n",
    "        if len(temp) > 0:\n",
    "            spl.append(temp)\n",
    "        \n",
    "    return spl\n",
    "    \n",
    "#splitsies(input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae455c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sentence 1:\\n{input_data[0]}\\nLabel: {label[0]}')\n",
    "type(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "token_text = []\n",
    "for para in input_data:\n",
    "    token_text.append(sent_tokenize(para))\n",
    "token_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b118f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f459c66",
   "metadata": {},
   "source": [
    "# Add Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6da5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = Perturb.perturb(input_data, Perturb.add_typos)\n",
    "ret.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79694d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just typos one sentence in input paragraph\n",
    "typo = []\n",
    "for thing in ret.data:\n",
    "    typo.append(thing[1])\n",
    "typo[0]\n",
    "#typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If \"sentence\" in para is only 1 char, add \"..\" to avoid errors\n",
    "for i in range(len(token_text)):\n",
    "    for j in range(len(token_text[i])):\n",
    "        if len(token_text[i][j]) <= 1:\n",
    "            token_text[i][j] = token_text[i][j]+'..'\n",
    "            \n",
    "token_text[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typo every sentence in every paragraph\n",
    "typoed = []\n",
    "lala = []\n",
    "for i in range(len(token_text)):\n",
    "    ret = Perturb.perturb(token_text[i], Perturb.add_typos)\n",
    "    typos = []\n",
    "    for sent in ret.data:\n",
    "        typos.append(sent[1])\n",
    "        lala.append(sent[1])\n",
    "    typoed.append(typos)\n",
    "typoed[6]\n",
    "#typoed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to list of paragraphs WOWWWW\n",
    "tp = []\n",
    "for i in range(len(typoed)):\n",
    "    para = \"\"\n",
    "    for j in range(len(typoed[i])):\n",
    "        para = para + typoed[i][j] + \" \"\n",
    "    tp.append(para)\n",
    "#tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510097fa",
   "metadata": {},
   "source": [
    "# POS tag data\n",
    "\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "When you call nlp on a text, spaCy will tokenize it and then call each component on the Doc, in order. It then returns the processed Doc that you can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = list(nlp.pipe(tp))\n",
    "#for doc in nlp.pipe(input_data):\n",
    "    # Do something with the doc here\n",
    "#    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c540f9",
   "metadata": {},
   "source": [
    "# Remove End Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa950bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata[0], Perturb.strip_punctuation(pdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = Perturb.perturb(pdata, Perturb.punctuation)\n",
    "no_punct = []\n",
    "for i in ret.data:\n",
    "    no_punct.append(i[1])\n",
    "no_punct[0]\n",
    "len(no_punct)\n",
    "no_punct[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afab46f",
   "metadata": {},
   "source": [
    "# Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02351295",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = list(nlp.pipe(no_punct))\n",
    "pdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This negates only 44 sentences\n",
    "nega = []\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    ret = Perturb.remove_negation(pdata[i])\n",
    "    if ret == None:  # if nothing changes\n",
    "        nega.append(pdata[i]) # append original sentence\n",
    "    else: \n",
    "        nega.append(ret) # append negated paragraph\n",
    "        count +=1\n",
    "len(nega)\n",
    "\n",
    "# Negation doesn't really change the sentiment labels, so they remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77766f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117db530",
   "metadata": {},
   "outputs": [],
   "source": [
    "nega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cc3de",
   "metadata": {},
   "source": [
    "# Make Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(len(input_data)):\n",
    "    dicti = {}\n",
    "    dicti['reviewText'] = str(nega[i])\n",
    "    dicti['sentiment'] = labels[i]\n",
    "    dicti['category'] = \"{'typos', 'punct', 'negation'}\"\n",
    "    output.append(dicti)\n",
    "len(output)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_json = [json.dumps(i)+'\\n' for i in output]\n",
    "#with open ('../data/predictions/dee_dump.json', 'w') as file:\n",
    "#    file.writelines(test_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
