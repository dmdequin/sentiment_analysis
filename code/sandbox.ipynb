{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic reqs\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# pre processing reqs \n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "# model reqs\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(path, N):\n",
    "    '''\n",
    "    input: path to json file\n",
    "    output: bert encodings\n",
    "    JSON -> df -> bert tokens -> bert embeddings\n",
    "    '''\n",
    "    df = pd.read_json(path, lines=True)\n",
    "\n",
    "    df['concatSummaryReview'] = df['summary'] + ' ' + df['reviewText']\n",
    "    df['concatSummaryReview'] = df['concatSummaryReview'].str.lower().fillna('[NA]')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # get the labels\n",
    "    sentiment_binary = {\n",
    "    'positive' : 1,\n",
    "    'negative' : 0\n",
    "    }\n",
    "\n",
    "    df['sentiment_binary'] = df['sentiment'].map(sentiment_binary)\n",
    "    labels = torch.tensor(df['sentiment_binary'])\n",
    "\n",
    "    # lists to store outputs of bert tokeniser \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    MAX_LEN = 512\n",
    "    BATCH_SIZE = 16 # For fine-tuning BERT, the authors recommend a batch size of 16 or 32\n",
    "\n",
    "    for i in range(N):\n",
    "        text = df['concatSummaryReview'][i]\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        encoded_plus = tokenizer.encode_plus(\n",
    "            text=tokens,                  # Preprocess sentence\n",
    "            add_special_tokens=True,    # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,         # Max length to truncate/pad\n",
    "            padding='max_length',       # Pad sentence to max length\n",
    "            return_attention_mask=True, # Return attention mask\n",
    "            truncation = True\n",
    "            # return_tensors='pt',        # Return PyTorch tensor\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_plus.get('input_ids'))\n",
    "        attention_masks.append(encoded_plus.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    # from here will be different for the test data\n",
    "    # and for the sata used for predictions - might need to split into 2 functions\n",
    "\n",
    "    # Create the DataLoader to be used as input in the model\n",
    "    labels = labels[0:N] # to limit the labels for testing purposes\n",
    "    data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    loader = DataLoader(data, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return loader, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    '''Bert Model for Classification Tasks.\n",
    "    '''\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        '''\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        '''\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        '''\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size, max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size, num_labels)\n",
    "        '''\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# defining optimiser\n",
    "def initialize_model(epochs=4):\n",
    "    '''\n",
    "    Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    '''\n",
    "    from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "    \n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_labels) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    import random\n",
    "    '''\n",
    "    Set seed for reproducibility.\n",
    "    '''\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# Specify loss function\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    '''\n",
    "    Train the BertClassifier model.\n",
    "    '''\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = LOSS_FN(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    '''\n",
    "    inputs: model and dataset to be evaluated\n",
    "    outputs:list of losses (?) and accuracy\n",
    "\n",
    "    After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    '''\n",
    "\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = LOSS_FN(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, loader):\n",
    "    '''Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    '''\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in loader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/raw/music_reviews_train.json'\n",
    "DEV = '../data/raw/music_reviews_dev.json'\n",
    "TEST = '../data/raw/music_reviews_test_masked.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.43 s, sys: 319 ms, total: 3.75 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# N = df.shape[0]\n",
    "N = 100\n",
    "\n",
    "train_loader, train_labels = pre_process(TRAIN, N)\n",
    "dev_loader, dev_labels = pre_process(DEV, N)\n",
    "test_loader, test_labels = pre_process(TEST, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which device to run this on\n",
    "# if torch.cuda.is_available():       \n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "#     print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# else:\n",
    "#     print('No GPU available, using the CPU instead.')\n",
    "#     device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 s, sys: 550 ms, total: 2.59 s\n",
      "Wall time: 3.62 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabrina/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(bert_classifier, train_loader, dev_loader, epochs=EPOCHS, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "\n",
    "model = bert_classifier\n",
    "pickle.dump(model, open('sab_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46445876, 0.5355412 ],\n",
       "       [0.47242603, 0.527574  ],\n",
       "       [0.46377763, 0.53622234],\n",
       "       [0.43349838, 0.56650156],\n",
       "       [0.43608373, 0.5639162 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre process test for prediction\n",
    "\n",
    "# predict on saved model\n",
    "model = pickle.load(open('sab_model.pkl', 'rb'))\n",
    "bert_predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Break it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# checklist for perturbations\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_train = pd.read_json('../data/raw/music_reviews_train.json', lines=True)\n",
    "\n",
    "input_data = list(data_train['reviewText'])[0:1000]\n",
    "input_data_labels = list(data_train['sentiment'])[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This tape can hardly be understood and it was listed for sale as \"very good\".  It\\'s VERY BAD.',\n",
       " 'negative')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[1], input_data_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sentences with Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['This album is terrible.', 'This album is terrible.',\n",
       "       'This book is terrible.', 'This album is great.',\n",
       "       'This song is fantastic.'], dtype='<U26')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = Editor()\n",
    "ret = editor.template('This {obj} is {adj}.',\n",
    "                       adj=['great', 'terrible', 'fantastic'],\n",
    "                       obj = ['book', 'song', 'album', 'product']\n",
    "                       )\n",
    "np.random.choice(ret.data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Niels went to see Søren in Viborg.',\n",
       " 'Alexander went to see Ove in Esbjerg.',\n",
       " 'Simon went to see Steen in Viborg.',\n",
       " 'Peter went to see Johannes in Hørsholm.',\n",
       " 'Aage went to see Ulrik in Esbjerg.',\n",
       " 'Valdemar went to see Jonas in Rønne.',\n",
       " 'Børge went to see Adam in Fredericia.',\n",
       " 'Søren went to see Flemming in Helsingør.',\n",
       " 'Leif went to see Carl in Kolding.',\n",
       " 'Henning went to see Børge in Sønderborg.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = Editor(language='danish')\n",
    "ret = editor.template('{male1} went to see {male2} in {city}.', remove_duplicates=True)\n",
    "list(np.random.choice(ret.data, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding perturbations \n",
    "Typos and stripping punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pdata = list(nlp.pipe(input_data)) # need this to strip punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>typos</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>no_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So creative!  Love his music - the words, the ...</td>\n",
       "      <td>So creative!  Love his music - the words, the ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>So creative!  Love his music - the words, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tape can hardly be understood and it was ...</td>\n",
       "      <td>This tape can hardly be understood and it was ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This tape can hardly be understood and it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buy the CD.  Do not buy the MP3 album.  Downlo...</td>\n",
       "      <td>Buy the CD.  Do not buy the MP3 album.  Downlo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Buy the CD.  Do not buy the MP3 album.  Downlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love Dallas Holms music and voice!  Thank Yo...</td>\n",
       "      <td>I lvoe Dallas Holms music and voice!  Thank Yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I love Dallas Holms music and voice!  Thank Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great memories of my early years in Christ</td>\n",
       "      <td>Great memories of mye arly years in Christ</td>\n",
       "      <td>positive</td>\n",
       "      <td>Great memories of my early years in Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>A friend in my husband local band back in the ...</td>\n",
       "      <td>A friend in my husband local band back in the ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A friend in my husband local band back in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>this is a New copy of a old song hated it. Wil...</td>\n",
       "      <td>this is a New copy of a old song hated it. Wil...</td>\n",
       "      <td>negative</td>\n",
       "      <td>this is a New copy of a old song hated it. Wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>This is a very nice song. Well performed by Na...</td>\n",
       "      <td>This is a very nice song. Well performed by Na...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This is a very nice song. Well performed by Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Great songs. Originals. No re-records.</td>\n",
       "      <td>Great songs. Origianls. No re-records.</td>\n",
       "      <td>positive</td>\n",
       "      <td>Great songs. Originals. No re-records</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I thought I wanted this, but I got sick of it ...</td>\n",
       "      <td>I thought I wanted this, but I gots ick of it ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I thought I wanted this, but I got sick of it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              original  \\\n",
       "0    So creative!  Love his music - the words, the ...   \n",
       "1    This tape can hardly be understood and it was ...   \n",
       "2    Buy the CD.  Do not buy the MP3 album.  Downlo...   \n",
       "3    I love Dallas Holms music and voice!  Thank Yo...   \n",
       "4           Great memories of my early years in Christ   \n",
       "..                                                 ...   \n",
       "995  A friend in my husband local band back in the ...   \n",
       "996  this is a New copy of a old song hated it. Wil...   \n",
       "997  This is a very nice song. Well performed by Na...   \n",
       "998             Great songs. Originals. No re-records.   \n",
       "999  I thought I wanted this, but I got sick of it ...   \n",
       "\n",
       "                                                 typos sentiment  \\\n",
       "0    So creative!  Love his music - the words, the ...  positive   \n",
       "1    This tape can hardly be understood and it was ...  negative   \n",
       "2    Buy the CD.  Do not buy the MP3 album.  Downlo...  negative   \n",
       "3    I lvoe Dallas Holms music and voice!  Thank Yo...  positive   \n",
       "4           Great memories of mye arly years in Christ  positive   \n",
       "..                                                 ...       ...   \n",
       "995  A friend in my husband local band back in the ...  positive   \n",
       "996  this is a New copy of a old song hated it. Wil...  negative   \n",
       "997  This is a very nice song. Well performed by Na...  positive   \n",
       "998             Great songs. Origianls. No re-records.  positive   \n",
       "999  I thought I wanted this, but I gots ick of it ...  negative   \n",
       "\n",
       "                                              no_punct  \n",
       "0    So creative!  Love his music - the words, the ...  \n",
       "1    This tape can hardly be understood and it was ...  \n",
       "2    Buy the CD.  Do not buy the MP3 album.  Downlo...  \n",
       "3    I love Dallas Holms music and voice!  Thank Yo...  \n",
       "4           Great memories of my early years in Christ  \n",
       "..                                                 ...  \n",
       "995  A friend in my husband local band back in the ...  \n",
       "996  this is a New copy of a old song hated it. Wil...  \n",
       "997  This is a very nice song. Well performed by Na...  \n",
       "998              Great songs. Originals. No re-records  \n",
       "999  I thought I wanted this, but I got sick of it ...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typos = Perturb.perturb(input_data, Perturb.add_typos)\n",
    "punct = Perturb.perturb(pdata, Perturb.strip_punctuation)\n",
    "\n",
    "punct_df = pd.DataFrame(punct['data'], columns=['original', 'no_punct'])\n",
    "\n",
    "pert = pd.DataFrame(typos.data, columns=['original', 'typos'])\n",
    "pert['sentiment'] = input_data_labels\n",
    "pert['no_punct'] = punct_df['no_punct']\n",
    "pert"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45d78ff8e39f9cb4612f96e0a0daed02b89d297fe7734681d306e9497b71623e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
