{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a105f6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b379ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ff764",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6981440",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/interim/train.csv'\n",
    "DEV   = '../data/interim/dev.csv'\n",
    "TEST  = '../data/interim/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a12e91",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a97f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(PATH):\n",
    "    with open(PATH, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        text = []\n",
    "        for lines in csvFile:\n",
    "            text.append(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def splitter(L):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in L:\n",
    "        X.append(i[0])\n",
    "        y.append(i[1])\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06495b8f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43eb346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loader(TRAIN) # Training\n",
    "dev_data = loader(DEV)     # Validation\n",
    "X_test = loader(TEST)      # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c526e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870f2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!',\n",
       " '1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad3020",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe51fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    \"\"\"Function to find all tokens in a given sentence\n",
    "    \"\"\"\n",
    "    tok = re.compile('[\\'\\\"]|[A-Za-z]+|[.?!:\\'\\\"]+')\n",
    "    \n",
    "    return tok.findall(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea7ecd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = splitter(train_data)\n",
    "X_dev, y_dev = splitter(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e915d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gotta', 'listen', 'to', 'this', '!', 'So', 'creative', '!', 'Love', 'his', 'music', 'the', 'words', 'the', 'message', '!', 'Some', 'of', 'my', 'favorite', 'songs', 'on', 'this', 'CD', '.', 'I', 'should', 'have', 'bought', 'it', 'years', 'ago', '!']\n"
     ]
    }
   ],
   "source": [
    "# hand-made tokenization\n",
    "print(tokenizer(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89f2a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "X_train_tokens = []\n",
    "for sentence in X_train:\n",
    "    temp = tokenizer(sentence)\n",
    "    if len(temp) > 0:\n",
    "        if len(temp) > 500:\n",
    "            X_train_tokens.append(temp[0:500])\n",
    "        else: X_train_tokens.append(temp)\n",
    "    else: X_train_tokens.append('NULL')\n",
    "print(len(X_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5124a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train_tokens:\n",
    "    if len(x) == 0 or len(x) > 500:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b76d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_train_tokens:\n",
    "    if i == []:\n",
    "        print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf56264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_dev_tokens = []\n",
    "for sentence in X_dev:\n",
    "    X_dev_tokens.append(tokenizer(sentence))\n",
    "print(len(X_dev_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "647f43bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ok ok'],\n",
       " 'Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4ac281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_test_tokens = []\n",
    "for sentence in X_test:\n",
    "    X_test_tokens.append(tokenizer(str(sentence)))\n",
    "print(len(X_test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871900c",
   "metadata": {},
   "source": [
    "## Compare with library tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59e00c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer\n",
    "#tokzr = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4535a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer tokenization\n",
    "#print(tokzr.tokenize(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24417e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4120bc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 2060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e25e321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            padding='max_length',         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation = True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b06be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode our concatenated data\n",
    "encoded_ = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65aebe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for sent in encoded_:\n",
    "    if len(sent) > l:\n",
    "        l = len(sent)\n",
    "        \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c67030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!\n",
      "Token IDs:  tensor([[  101, 10657,  4952,  ...,     0,     0,     0],\n",
      "        [  101,  9467,  9467,  ...,     0,     0,     0]])\n",
      "Tokenizing data...\n",
      "F'ing Done!!\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = l\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = preprocessing_for_bert(X_train[0:2])\n",
    "print('Original: ', X_train[0])\n",
    "print('Token IDs: ', token_ids[0])\n",
    "\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_dev)\n",
    "print('F\\'ing Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1be07fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000, 10000, 10000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(train_masks), len(val_inputs), len(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb63408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 10657,  4952,  ...,     0,     0,     0],\n",
       "        [  101,  9467,  9467,  ...,     0,     0,     0],\n",
       "        [  101,  4965,  1996,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2274,  3340,  ...,     0,     0,     0],\n",
       "        [  101,  2919,  3617,  ...,     0,     0,     0],\n",
       "        [  101,  1012,  1012,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e555c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cff316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
