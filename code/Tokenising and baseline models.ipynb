{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a105f6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b379ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ff764",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6981440",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/interim/train.csv'\n",
    "DEV   = '../data/interim/dev.csv'\n",
    "TEST  = '../data/interim/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a12e91",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a97f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(PATH):\n",
    "    with open(PATH, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        text = []\n",
    "        for lines in csvFile:\n",
    "            text.append(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def splitter(L):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in L:\n",
    "        X.append(i[0])\n",
    "        y.append(int(i[1]))\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06495b8f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43eb346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loader(TRAIN) # Training\n",
    "dev_data = loader(DEV)     # Validation\n",
    "X_test = loader(TEST)      # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c526e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc804ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870f2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!',\n",
       " '1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad3020",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe51fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    \"\"\"Function to find all tokens in a given sentence\n",
    "    \"\"\"\n",
    "    tok = re.compile('[\\'\\\"]|[A-Za-z]+|[.?!:\\'\\\"]+')\n",
    "    \n",
    "    return tok.findall(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7ecd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = splitter(train_data)\n",
    "X_dev, y_dev = splitter(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e915d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gotta', 'listen', 'to', 'this', '!', 'So', 'creative', '!', 'Love', 'his', 'music', 'the', 'words', 'the', 'message', '!', 'Some', 'of', 'my', 'favorite', 'songs', 'on', 'this', 'CD', '.', 'I', 'should', 'have', 'bought', 'it', 'years', 'ago', '!']\n"
     ]
    }
   ],
   "source": [
    "# hand-made tokenization\n",
    "print(tokenizer(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f2a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "X_train_tokens = []\n",
    "for sentence in X_train:\n",
    "    temp = tokenizer(sentence)\n",
    "    if len(temp) > 0:\n",
    "        if len(temp) > 500:\n",
    "            X_train_tokens.append(temp[0:500])\n",
    "        else: X_train_tokens.append(temp)\n",
    "    else: X_train_tokens.append('NULL')\n",
    "print(len(X_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5124a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train_tokens:\n",
    "    if len(x) == 0 or len(x) > 500:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b76d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_train_tokens:\n",
    "    if i == []:\n",
    "        print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbf56264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_dev_tokens = []\n",
    "for sentence in X_dev:\n",
    "    X_dev_tokens.append(tokenizer(sentence))\n",
    "print(len(X_dev_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647f43bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ok ok'],\n",
       " 'Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ac281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_test_tokens = []\n",
    "for sentence in X_test:\n",
    "    X_test_tokens.append(tokenizer(str(sentence)))\n",
    "print(len(X_test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee794c10",
   "metadata": {},
   "source": [
    "# Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4120bc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 2060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e25e321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            padding='max_length',         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation = True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b06be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode our concatenated data\n",
    "encoded_ = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65aebe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for sent in encoded_:\n",
    "    if len(sent) > l:\n",
    "        l = len(sent)\n",
    "        \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c67030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!\n",
      "Token IDs:  tensor([[  101, 10657,  4952,  2000,  2023,   999,  2061,  5541,   999,  2293,\n",
      "          2010,  2189,  1011,  1996,  2616,  1010,  1996,  4471,   999,  2070,\n",
      "          1997,  2026,  5440,  2774,  2006,  2023,  3729,  1012,  1045,  2323,\n",
      "          2031,  4149,  2009,  2086,  3283,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9467,  9467,  2023,  6823,  2064,  6684,  2022,  5319,  1998,\n",
      "          2009,  2001,  3205,  2005,  5096,  2004,  1000,  2200,  2204,  1000,\n",
      "          1012,  2009,  1005,  1055,  2200,  2919,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]])\n",
      "Tokenizing data...\n",
      "F'ing Done!!\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = l\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = preprocessing_for_bert(X_train[0:2])\n",
    "print('Original: ', X_train[0])\n",
    "print('Token IDs: ', token_ids[0])\n",
    "\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_dev)\n",
    "print('F\\'ing Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1be07fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10000, 10000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(train_masks), len(val_inputs), len(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb63408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 10657,  4952,  2000,  2023,   999,  2061,  5541,   999,  2293,\n",
       "          2010,  2189,  1011,  1996,  2616,  1010,  1996,  4471,   999,  2070,\n",
       "          1997,  2026,  5440,  2774,  2006,  2023,  3729,  1012,  1045,  2323,\n",
       "          2031,  4149,  2009,  2086,  3283,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  9467,  9467,  2023,  6823,  2064,  6684,  2022,  5319,  1998,\n",
       "          2009,  2001,  3205,  2005,  5096,  2004,  1000,  2200,  2204,  1000,\n",
       "          1012,  2009,  1005,  1055,  2200,  2919,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  4965,  1996,  3729,  1012,  2079,  2025,  4965,  1996, 23378,\n",
       "          1012,  4965,  1996,  3729,  1012,  2079,  2025,  4965,  1996, 23378,\n",
       "          2201,  1012,  8816,  2003,  2053,  2936,  2800,  1012,  2021,  2017,\n",
       "          2123,  1005,  1056,  2424,  2008,  2041,  2127,  2044,  2017,  2031,\n",
       "          4156,  2009,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2274,  3340,  1045,  2293,  5759, 28925,  2015,  2189,  1998,\n",
       "          2376,   999,  4067,  2017,   999,  1045,  2097,  2022,  7052,  2035,\n",
       "          2010,  6759,  1999,  6014,  1010,  5091,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2274,  3340,  2307,  5758,  1997,  2026,  2220,  2086,  1999,\n",
       "          4828,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  4086,  5544,  1012,  1012,  1012,  1045,  2031,  2042,  5962,\n",
       "          2000,  2023,  2201,  2275,  2026,  2972,  2166,  1006,  2382,  2086,\n",
       "           999,  1007,  1045,  3342,  4994,  2023,  3729,  2006,  9377,  2296,\n",
       "          2305,  2004,  1037,  6927,  3917,  1010,  1998,  2009,  2716,  2033,\n",
       "          7216,  2000,  3637,  1010,  2004,  1045,  2467,  2018,  2000,  2031,\n",
       "          2070,  2189,  2030,  1037,  5470,  2000,   102],\n",
       "        [  101,  2274,  3340,  1045,  2293,  2035,  1997,  2010,  2189,   999,\n",
       "           999,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2293, 14838,  2189,  2200, 18988,  1998,  2144,  2023,  2001,\n",
       "          1037,  1012,  1012,  1012,  2293, 14838,  2189,  2200, 18988,  1998,\n",
       "          2144,  2023,  2001,  1037,  5592,  1010,  1045,  2074,  2031,  2000,\n",
       "          4965,  2009,  2005,  2870,  2574,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2087,  4310,  2614,  1012,  2498,  2130,  2714,  2426,  8489,\n",
       "          2189,  1012,  1012,  1012,  2087,  4310,  2614,  1012,  2498,  2130,\n",
       "          2714,  2426,  8489,  2189,  9804,  1012,  5621,  2613,  1998,  7425,\n",
       "          3993,  1012,  1045,  2572,  7727,  2069,  2000,  1996,  3080,  1000,\n",
       "          1996,  8140, 15264,  1000,  2112,  8402,  2000,  2650,  1023,  2030,\n",
       "          2061,  1012,   102,     0,     0,     0,     0],\n",
       "        [  101,  3729,  3319,  1996,  8875,  3641,  3369,  1999,  1037, 23259,\n",
       "          5450,  1012,  1045,  2001,  3243,  7622,  1012,  1996,  3737,  1997,\n",
       "          1996,  3729,  1013,  2189,  1045, 10392,  1012,  1045,  2572,  2200,\n",
       "          8510,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e555c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_dev)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fafd2",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "792aed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 ms, sys: 641 µs, total: 26.6 ms\n",
      "Wall time: 24.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size, max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a605a",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc2f3336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 10:49:54.403698: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-22 10:49:54.403750: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "492644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4cb85",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13544696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/xanadoo/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |    0    |   0.636953   |     -      |     -     |   0.57   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.636953   |  0.677477  |   56.82   |  123.56  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |    0    |   0.550048   |     -      |     -     |   0.66   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.550048   |  0.678568  |   56.82   |  125.01  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1d600e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(X_test_tokens)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9518b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "829e7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6afd461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5319529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7249\n",
      "Accuracy: 56.83%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcdf7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ad1bc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets predicted non-negative:  7809\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the probabilities\n",
    "threshold = 0.6\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of tweets predicted non-negative: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ac8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
