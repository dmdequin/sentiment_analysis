{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a105f6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b379ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9438dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_29032022_111303\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"_%d%m%Y_%H%M%S\")\n",
    "print('this'+current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ff764",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6981440",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/interim/train.csv'\n",
    "DEV   = '../data/interim/dev.csv'\n",
    "TEST  = '../data/interim/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a12e91",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a97f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(PATH):\n",
    "    with open(PATH, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        text = []\n",
    "        for lines in csvFile:\n",
    "            text.append(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def splitter(L):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in L:\n",
    "        X.append(i[0])\n",
    "        y.append(int(i[1]))\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe51fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tknzr(sentence):\n",
    "    \"\"\"Function to find all tokens in a given sentence\n",
    "    \"\"\"\n",
    "    tok = re.compile('[\\'\\\"]|[A-Za-z]+|[.?!:\\'\\\"]+')\n",
    "    \n",
    "    return tok.findall(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee794c10",
   "metadata": {},
   "source": [
    "# Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4120bc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a6b35",
   "metadata": {},
   "source": [
    "# Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c319164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06495b8f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43eb346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loader(TRAIN) # Training\n",
    "dev_data = loader(DEV)     # Validation\n",
    "X_test = loader(TEST)      # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c526e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10000, 10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37260ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to select a subset of the data\n",
    "train_data = train_data[0:5] \n",
    "dev_data = dev_data[0:10]\n",
    "X_test = X_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870f2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!',\n",
       " '1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad3020",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea7ecd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = splitter(train_data)\n",
    "X_dev, y_dev = splitter(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e915d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gotta', 'listen', 'to', 'this', '!', 'So', 'creative', '!', 'Love', 'his', 'music', 'the', 'words', 'the', 'message', '!', 'Some', 'of', 'my', 'favorite', 'songs', 'on', 'this', 'CD', '.', 'I', 'should', 'have', 'bought', 'it', 'years', 'ago', '!']\n"
     ]
    }
   ],
   "source": [
    "# hand-made tokenization\n",
    "print(tknzr(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f2a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gotta',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'this',\n",
       " '!',\n",
       " 'So',\n",
       " 'creative',\n",
       " '!',\n",
       " 'Love',\n",
       " 'his',\n",
       " 'music',\n",
       " 'the',\n",
       " 'words',\n",
       " 'the',\n",
       " 'message',\n",
       " '!',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'songs',\n",
       " 'on',\n",
       " 'this',\n",
       " 'CD',\n",
       " '.',\n",
       " 'I',\n",
       " 'should',\n",
       " 'have',\n",
       " 'bought',\n",
       " 'it',\n",
       " 'years',\n",
       " 'ago',\n",
       " '!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokens = []\n",
    "for sentence in X_train:\n",
    "    temp = tknzr(sentence)\n",
    "    if len(temp) > 0:\n",
    "        if len(temp) > 500:\n",
    "            X_train_tokens.append(temp[0:500])\n",
    "        else: X_train_tokens.append(temp)\n",
    "    else: X_train_tokens.append('NULL')\n",
    "print(len(X_train_tokens))\n",
    "X_train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5124a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train_tokens:\n",
    "    if len(x) == 0 or len(x) > 500:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b76d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_train_tokens:\n",
    "    if i == []:\n",
    "        print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbf56264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "X_dev_tokens = []\n",
    "for sentence in X_dev:\n",
    "    X_dev_tokens.append(tknzr(sentence))\n",
    "print(len(X_dev_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647f43bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ok ok'],\n",
       " 'Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ac281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "X_test_tokens = []\n",
    "for sentence in X_test:\n",
    "    X_test_tokens.append(tknzr(str(sentence)))\n",
    "print(len(X_test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e25e321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,      # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,           # Max length to truncate/pad\n",
    "            padding='max_length',         # Pad sentence to max length\n",
    "            #return_tensors='pt',         # Return PyTorch tensor\n",
    "            return_attention_mask=True,   # Return attention mask\n",
    "            truncation = True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48b06be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 10657, 4952, 2000, 2023, 999, 2061, 5541, 999, 2293, 2010, 2189, 1011, 1996, 2616, 1010, 1996, 4471, 999, 2070, 1997, 2026, 5440, 2774, 2006, 2023, 3729, 1012, 1045, 2323, 2031, 4149, 2009, 2086, 3283, 999, 102], [101, 9467, 9467, 2023, 6823, 2064, 6684, 2022, 5319, 1998, 2009, 2001, 3205, 2005, 5096, 2004, 1000, 2200, 2204, 1000, 1012, 2009, 1005, 1055, 2200, 2919, 1012, 102]]\n"
     ]
    }
   ],
   "source": [
    "# Encode our concatenated data\n",
    "encoded_ = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]\n",
    "print(encoded_[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65aebe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for sent in encoded_:\n",
    "    if len(sent) > l:\n",
    "        l = len(sent)\n",
    "        \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c67030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!\n",
      "Token IDs:  tensor([[  101, 10657,  4952,  2000,  2023,   999,  2061,  5541,   999,  2293,\n",
      "          2010,  2189,  1011,  1996,  2616,  1010,  1996,  4471,   999,  2070,\n",
      "          1997,  2026,  5440,  2774,  2006,  2023,  3729,  1012,  1045,  2323,\n",
      "          2031,  4149,  2009,  2086,  3283,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  9467,  9467,  2023,  6823,  2064,  6684,  2022,  5319,  1998,\n",
      "          2009,  2001,  3205,  2005,  5096,  2004,  1000,  2200,  2204,  1000,\n",
      "          1012,  2009,  1005,  1055,  2200,  2919,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "Tokenizing data...\n",
      "F'ing Done!!\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = l\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = preprocessing_for_bert(X_train[0:2])\n",
    "print('Original: ', X_train[0])\n",
    "print('Token IDs: ', token_ids[0])\n",
    "\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_dev)\n",
    "print('F\\'ing Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1be07fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 10, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(train_masks), len(val_inputs), len(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb63408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 10657,  4952,  2000,  2023,   999,  2061,  5541,   999,  2293,\n",
       "         2010,  2189,  1011,  1996,  2616,  1010,  1996,  4471,   999,  2070,\n",
       "         1997,  2026,  5440,  2774,  2006,  2023,  3729,  1012,  1045,  2323,\n",
       "         2031,  4149,  2009,  2086,  3283,   999,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e555c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_dev)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fafd2",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "792aed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 46.9 ms, total: 93.8 ms\n",
      "Wall time: 78.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size, max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a605a",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc2f3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "492644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into evaluation mode. Dropout layers are disabled during test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4cb85",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13544696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/dmdequin/miniconda3/envs/uni/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |    2    |   0.680853   |     -      |     -     |   7.65   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.680853   |  0.695005  |   40.00   |   8.20   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |    2    |   0.565419   |     -      |     -     |   3.15   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.565419   |  0.684613  |   60.00   |   3.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "CPU times: user 33.7 s, sys: 21.2 s, total: 54.9 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945586e",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea574235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model = bert_classifier\n",
    "pickle.dump(model, open('model' + current_time + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = pickle.load(open('model' + current_time + '.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f676fb6",
   "metadata": {},
   "source": [
    "# Run Preprocessing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1d600e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and embed data...\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing and embed data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(X_test_tokens)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6189151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into evaluation mode. Dropout layers are disabled during testing.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71748c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdcf25",
   "metadata": {},
   "source": [
    "# Predict Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2f1db9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.62 s, sys: 1.2 s, total: 4.83 s\n",
      "Wall time: 906 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute predicted probabilities on the validation set\n",
    "probs = bert_predict(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6acb3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews predicted positive:  0\n",
      "Accuracy: 60.0%\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the probabilities\n",
    "threshold = 0.63\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of reviews predicted positive: \", preds.sum())\n",
    "\n",
    "print(f'Accuracy: {round(100 * sum(preds==y_dev)/len(y_dev), 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d22919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7083\n",
      "Accuracy: 60.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAx5ElEQVR4nO3dd3wUdf7H8ddHehMUEE9R4XcgAgpREBsoinLY9WzYOD09K3Y9++nZ29mxIHqoKKicBSucInZPURFCFaVFQBEbRaR9fn98J2aJyWZJMjvJ5v18PPaRnZ3Zmc9Okvns9zszn6+5OyIiIqXZIOkARESkalOiEBGRtJQoREQkLSUKERFJS4lCRETSUqIQEZG0lChkvZjZZDPrnXQcVYWZXWZmQxLa9lAzuy6JbVc2MzvWzMaU8736m4yZEkU1ZmazzewXM1tqZgujA0fjOLfp7p3dfVyc2yhkZvXM7EYzmxt9zi/M7CIzs2xsv4R4eptZQepr7n6Du58c0/bMzM42s3wzW2ZmBWb2jJltF8f2ysvMrjazYRVZh7s/4e59M9jW75JjNv8mayoliurvQHdvDOQB2wOXJhvO+jOz2qXMegboA+wHNAGOB04B7oohBjOzqvb/cBdwDnA2sDGwNfA8sH9lbyjN7yB2SW5bMuTuelTTBzAb2Dtl+hbg5ZTpnYH3gR+Bz4HeKfM2Bv4NzAd+AJ5PmXcAMCF63/tAl+LbBDYDfgE2Tpm3PfAdUCea/iswNVr/aGCrlGUdOBP4AphVwmfrA6wAtij2+k7AGqBdND0OuBH4CPgJeKFYTOn2wTjgeuC96LO0A06MYl4CfAWcGi3bKFpmLbA0emwGXA0Mi5ZpE32uvwBzo31xecr2GgCPRvtjKvB3oKCU32376HP2SPP7HwoMAl6O4v0f8MeU+XcB84CfgU+AXinzrgZGAsOi+ScDPYAPon21ALgXqJvyns7Af4HvgW+Ay4B+wEpgVbRPPo+WbQo8HK3na+A6oFY074Ron98Rreu66LV3o/kWzfs2+p1OBLYlfElYFW1vKfBi8f8DoFYU15fRPvmEYn9DepTjWJN0AHpU4Je37j9Ia2AScFc0vTmwmPBtfANgn2i6ZTT/ZeApYCOgDrBH9PoO0T/oTtE/3V+i7dQrYZtjgb+lxHMr8ED0/BBgJtARqA1cAbyfsqxHB52NgQYlfLabgLdK+dxzKDqAj4sORNsSDub/oejAXdY+GEc4oHeOYqxD+Lb+x+hgtQewHNghWr43xQ7slJwoHiIkha7Ar0DH1M8U7fPWhANgaYniNGBOGb//oYQDbY8o/ieAESnzjwOaR/MuABYC9VPiXhX9njaI4u1GSKy1o88yFTg3Wr4J4aB/AVA/mt6p+D5I2fbzwIPR72QTQiIv/J2dAKwGzoq21YB1E8WfCAf4ZtHvoSPwh5TPfF2a/4OLCP8HHaL3dgWaJ/2/Wt0fiQegRwV+eeEfZCnhm5MDbwDNonkXA48XW3404cD/B8I3441KWOf9wLXFXptOUSJJ/ac8GRgbPTfCt9fdo+lXgZNS1rEB4aC7VTTtwF5pPtuQ1INesXkfEn1TJxzsb0qZ14nwjbNWun2Q8t5rytjHzwPnRM97k1miaJ0y/yOgf/T8K+BPKfNOLr6+lHmXAx+WEdtQYEjK9H7AtDTL/wB0TYn77TLWfy7wXPT8aOCzUpb7bR9E060ICbJBymtHA29Gz08A5hZbxwkUJYq9gBmEpLVBCZ85XaKYDhxc0f8tPdZ9VLU+WVl/h7h7E8JBbBugRfT6VsARZvZj4QPoSUgSWwDfu/sPJaxvK+CCYu/bgtDNUtxIYBcz2wzYnXCQfCdlPXelrON7QjLZPOX989J8ru+iWEvyh2h+SeuZQ2gZtCD9PigxBjPb18w+NLPvo+X3o2ifZmphyvPlQOEFBpsV2166z7+Y0j9/JtvCzC4ws6lm9lP0WZqy7mcp/tm3NrOXogsjfgZuSFl+C0J3Tia2IvwOFqTs9wcJLYsSt53K3ccSur0GAd+Y2WAz2zDDba9PnJIhJYoc4e5vEb5t3Ra9NI/wbbpZyqORu98UzdvYzJqVsKp5wPXF3tfQ3YeXsM0fgTHAkcAxwHCPvtZF6zm12HoauPv7qatI85FeB3Yysy1SXzSzHoSDwdiUl1OX2ZLQpfJdGfvgdzGYWT1C19VtQCt3bwa8QkhwZcWbiQWELqeS4i7uDaC1mXUvz4bMrBehRXUkoeXYjNDfn3rFWPHPcz8wDWjv7hsS+voLl59H6JIrSfH1zCO0KFqk7PcN3b1zmvesu0L3u929G6FbcGtCl1KZ7ysjTiknJYrcciewj5nlEU5SHmhmfzKzWmZWP7q8s7W7LyB0Dd1nZhuZWR0z2z1ax0PAaWa2U3QlUCMz29/MmpSyzSeBAcBh0fNCDwCXmllnADNramZHZPpB3P11wsHyP2bWOfoMOxP64e939y9SFj/OzDqZWUPgGmCku69Jtw9K2WxdoB6wCFhtZvsCqZdsfgM0N7OmmX6OYp4m7JONzGxzYGBpC0af7z5geBRz3Sj+/mZ2SQbbakI4D7AIqG1m/wDK+lbehHBie6mZbQOcnjLvJWBTMzs3umy5iZntFM37BmhTeNVY9Pc1BviXmW1oZhuY2R/NbI8M4sbMdoz+/uoAywgXNaxJ2db/pXn7EOBaM2sf/f12MbPmmWxXSqdEkUPcfRHwGHClu88DDiZ8K1xE+KZ1EUW/8+MJ37ynEU5enxutYzzwN0LT/wfCCekT0mx2FOEKnW/c/fOUWJ4DbgZGRN0Y+cC+6/mRDgPeBF4jnIsZRriS5qxiyz1OaE0tJJxoPTuKoax9sA53XxK992nCZz8m+nyF86cBw4Gvoi6Vkrrj0rkGKABmEVpMIwnfvEtzNkVdMD8SulQOBV7MYFujCV8GZhC641aQvqsL4ELCZ15C+MLwVOGMaN/sAxxI2M9fAHtGs5+Jfi42s0+j5wMIiXcKYV+OJLOuNAgJ7aHofXMI3XCFLeWHgU7R/n++hPfeTvj9jSEkvYcJJ8ulAqyop0Ck+jGzcYQTqYncHV0RZnY64UR3Rt+0RZKiFoVIlpjZH8xst6grpgPhUtPnko5LpCyxJQoze8TMvjWz/FLmm5ndbWYzzWyime0QVywiVURdwtU/Swgn418gnIcQqdJi63qKTo4uBR5z921LmL8foa95P8LNXXe5+07FlxMRkWTF1qJw97cJ186X5mBCEnF3/xBoZmaZnuwSEZEsSbIY1+asexVGQfTaguILmtkphDovNGrUqNs222yTlQBF1sf06fDLL9BA19hIFdJ85QI2XrmQz1j7nbu3LM86kkwUJZWKLrEfzN0HA4MBunfv7uPHj48zLpFy6d07/Bw3LskoRCLuYAajRsGYMdigQXPKu6okr3oqYN07U1sTKpmKiEh5/fADnHQS3HBDmD7oILj33gqtMslEMQoYEF39tDPwU3RHp4iIlMdzz0GnTvDoo7BqVaWtNrauJzMbTihU18LCqGBXEQqF4e4PEGro7Ee483c5YRwAERFZX998A2edBc88A3l58PLLsEPl3XEQW6Jw96PLmO+EgWtERKQi5s0LyeH66+Gii6BOnUpdvYYgFBGpjubMgRdfhIEDoXt3mDsXmsdT/1AlPEREqpO1a2HQINh2W7j0UlgQndqNKUmAEoWISPUxfTrssUdoRey2G+Tnwx/iv09ZXU8iItXB8uXQsyesWQNDh8KAAeE+iSxQohARqcpmzID27aFhQ3j88XBV06abZjUEdT2JiFRFK1bA5ZeH+yKeeCK81q9f1pMEqEUhIlL1vPdeuLt6+nQ48UTYf/9Ew1GLQkSkKrn2WujVK7QoRo+GRx6BjTZKNCQlChGRqqBwbKC8vHCXdX4+9O2baEiFlChERJL0/ffwl7/AddeF6QMPhLvugsaNk40rhRKFiEhSRo6Ejh3hySeLWhRVkE5mi4hk24IF4aa5Z5+Fbt1gzBjo2jXpqEqlFoWISLbNnx9OVN98M3z4YZVOEqAWhYhIdsyeHYr4nXVWaEXMm5f41UyZUotCRCROa9bA3XeHIn6XXw4LF4bXq0mSACUKEZH4TJ0Ku+8O55wT7o3Iz0/kzuqKUteTiEgcli8PSWLtWnjsMTjuuKwV8atsShQiIpVp2jTo0CEU8XviiXCiulWrpKOqEHU9iYhUhl9+gYsvhs6di4r49e1b7ZMEqEUhIlJxb78NJ58MX3wRfh5wQNIRVSq1KEREKuKf/wyjzq1eDa+/Dg89BM2aJR1VpVKiEBEpj8KSG927w3nnwaRJ0KdPsjHFRIlCRGR9fPcdHH98KAcOYayI22+HRo2SjStGShQiIplwh6efDiPOjRgBG9Scw6dOZouIlGX+fDjjDHjhhdDV9Prr0KVL0lFlTc1JiSIi5bVwIYwdC7feCh98UKOSBKhFISJSsq++glGj4NxzYYcdYO7cnLuaKVNqUYiIpFqzBu64IxTxu+qqoiJ+NTRJgBKFiEiRyZNht93g/PNhr73CdDUs4lfZ1PUkIgKhiN8ee4TCfU8+Cf37V9sifpVNiUJEarYpU8K41Q0bhsteu3aFli2TjqpKUdeTiNRMy5fDRRfBdtvBsGHhtb33VpIogVoUIlLzjBsHf/sbzJwJp54KBx2UdERVmloUIlKzXHUV7LlnuNN67Fh44AFo2jTpqKo0JQoRqRkKi/j16AEXXAATJ4aEIWWKNVGYWT8zm25mM83skhLmNzWzF83sczObbGYnxhmPiNRAixbBMcfANdeE6f33h9tuCyevJSOxJQozqwUMAvYFOgFHm1mnYoudCUxx965Ab+BfZlY3rphEpAZxD5e5duwII0dCXR1ayivOFkUPYKa7f+XuK4ERwMHFlnGgiZkZ0Bj4HlgdY0wiUhMUFIQT1MceC+3awWefwaWXJh1VtRVnotgcmJcyXRC9lupeoCMwH5gEnOPua4uvyMxOMbPxZjZ+0aJFccUrIrli0aIwPOntt8N774VxrKXc4kwUJd3S6MWm/wRMADYD8oB7zWzD373JfbC7d3f37i11jbOIlGTmzFCjCWD77WHevDDyXK1aycaVA+JMFAXAFinTrQkth1QnAs96MBOYBWwTY0wikmtWrw4np7fbLoxf/c034fUNf/edU8opzkTxMdDezNpGJ6j7A6OKLTMX6ANgZq2ADsBXMcYkIrlk0iTYdddwh3XfvqGIX6tWSUeVc2K7M9vdV5vZQGA0UAt4xN0nm9lp0fwHgGuBoWY2idBVdbG7fxdXTCKSQ5YvD/dBbLBBqNF05JEq4heTWEt4uPsrwCvFXnsg5fl8oG+cMYhIjsnPDyenGzaEp54KRfxatEg6qpymO7NFpHpYtiyME9GlS1ERvz59lCSyQEUBRaTqe+ONUMRv1iw44ww4uPgtWRIntShEpGq78spQ/rt2bXjrLRg0SFc0ZZkShYhUTWuje2933RX+/nf4/HPYffdkY6qh1PUkWTN4cCi9k6smTIC8vKSjyAHffgtnnw0dOoT7IvbdNzwkMWpRSNY8+WQ4mOaqvLxQpFTKyT2cpO7YEZ57TtVdqxC1KCSr8vLC4GIi65g3D047DV55BXbZBYYMgU7Fi01LUtSiEJHkLV4civfddRe8846SRBWjFoWIJGPGDBg1Ci68MDQ1582DJk2SjkpKoBaFiGTX6tVw883hxrnrry8q4qckUWUpUYhI9nz+Oey0E1xyCey3H0yZoiJ+1YC6nkQkO5YvDyU3atcOQ5MedljSEUmGlChEJF4TJ4axIho2hGeeCUX8Nt446ahkPajrSUTisXQpnHNOOFH9+OPhtT33VJKohtSiEJHK99//wimnwOzZMHAgHHpo0hFJBahFISKV6/LLw2hz9eqFeyLuuUdXNFVzGScKM2sUZyAiUs0VFvHr2RMuvTTUa+nZM9GQpHKUmSjMbFczmwJMjaa7mtl9sUcmItXDwoVw+OFw9dVhet994YYboH79RMOSypNJi+IO4E/AYgB3/xxQrV+Rms4dhg4N5TZeekljROSwjE5mu/s8W3fQ8jXxhCMi1cKcOeFk9ZgxoXtpyJBQFlxyUiYtinlmtivgZlbXzC4k6oYSkRrqxx/h44/h3nvDqHNKEjktkxbFacBdwOZAATAGOCPOoESkCpo+PRTxu+iicNPc3LnQuHHSUUkWZNKi6ODux7p7K3ffxN2PAzrGHZiIVBGrVsGNN4bkcNNNYQQ6UJKoQTJJFPdk+JqI5JrPPgtF/C67DA48MBTx22STpKOSLCu168nMdgF2BVqa2fkpszYEasUdmIgkbPly2GcfqFMH/vMf+POfk45IEpLuHEVdoHG0TOptlT8Dh8cZlIgk6LPPQn2mhg1DldeuXWGjjZKOShJUaqJw97eAt8xsqLvPyWJMIpKEJUvCHdWDBsGjj8KAAdC7d9JRSRWQyVVPy83sVqAz8Nutlu6+V2xRiUh2vfYanHpqGI70nHPUzSTryORk9hPANKAt8E9gNvBxjDGJSDZdemkou9GoEbz3Htx5p65oknVk0qJo7u4Pm9k5Kd1Rb8UdmIjEbM0aqFUrdC/Vrg1XXBEqvooUk0miWBX9XGBm+wPzgdbxhSQisVqwAM48Ezp3hmuvhT/9KTxESpFJ19N1ZtYUuAC4EBgCnBtnUCISA3f4979DEb9XX9WVTJKxMlsU7v5S9PQnYE8AM9stzqBEpJLNng1/+xu8/jr06hWK+G29ddJRSTWR7oa7WsCRhBpPr7l7vpkdAFwGNAC2z06IIlJhP/0En34K990Xrm7aQINbSubS/bU8DJwMNAfuNrN/A7cBt7h7RknCzPqZ2XQzm2lml5SyTG8zm2Bmk3WSXKQSTZkSajNBURG/009XkpD1lq7rqTvQxd3Xmll94DugnbsvzGTFUYtkELAPoersx2Y2yt2npCzTDLgP6Ofuc81MRWREKmrlSrjllnCiukkT+OtfQ32mRhrNWMon3VeLle6+FsDdVwAzMk0SkR7ATHf/yt1XAiOAg4stcwzwrLvPjbbz7XqsX0SKGz8edtwRrrwy3DSnIn5SCdK1KLYxs4nRcwP+GE0b4O7epYx1bw7MS5kuAHYqtszWQB0zG0eoJ3WXuz9WfEVmdgpwCsCWW25ZxmZFaqhly8JlrvXrwwsvwEEHJR2R5Ih0iaKiY05YCa95CdvvBvQhnCD/wMw+dPcZ67zJfTAwGKB79+7F1yFSs336aSji16gRPPccdOkCzZolHZXkkFK7ntx9TrpHBusuALZImW5NuFmv+DKvufsyd/8OeBvour4fQqRG+vlnOOMM6NYNhg0Lr+2+u5KEVLo4L3/4GGhvZm3NrC7QHxhVbJkXgF5mVtvMGhK6pjQet0hZXnkl3Fn94INw/vlw2GFJRyQ5LJMSHuXi7qvNbCAwmjDQ0SPuPtnMTovmP+DuU83sNWAisBYY4u75ccUkkhMuvjhc1dSpUxgvYqfip/5EKldGicLMGgBbuvv09Vm5u78CvFLstQeKTd8K3Lo+6xWpcdxh7dpQxK9Pn3DC+rLLVMRPsqLMriczOxCYALwWTeeZWfEuJBGJy9dfwyGHwFVXhem+feGf/1SSkKzJ5BzF1YR7In4EcPcJQJu4AhKRiDs89FDoYhozBlq0SDoiqaEy6Xpa7e4/mZV0tauIxGLWLDjpJHjzzTBexEMPQbt2SUclNVQmiSLfzI4BaplZe+Bs4P14wxKp4ZYuhYkTw1VNJ5+s+kySqEz++s4ijJf9K/Akodz4uTHGJFIz5efDDTeE59ttF4r4nXKKkoQkLpO/wA7ufrm77xg9rohqP4lIZVi5Mpyc3mEHuOMO+DYqedawYbJxiUQySRS3m9k0M7vWzDrHHpFITfLxx+HO6quvhiOOUBE/qZIyGeFuTzPblDCI0WAz2xB4yt2viz06kVy2bBn06wcNGsCoUXDggUlHJFKijDo/3X2hu98NnEa4p+IfcQYlktPGjw83zzVqFKq8Tp6sJCFVWiY33HU0s6vNLB+4l3DFU+vYIxPJNT/9FIYh3XHHoiJ+PXtC06bJxiVShkwuj/03MBzo6+7Fq7+KSCZefBFOOw0WLoQLL4TDD086IpGMZXKOYudsBCKSsy66CG67LVzy+vzzoUUhUo2UmijM7Gl3P9LMJrHugEOZjnAnUnO5w5o1ULt2qM204Yah6mvduklHJrLe0rUozol+HpCNQERyRkEBnH56GGnu+uthn33CQ6SaSjfC3YLo6RkljG53RnbCE6lG1q4NJTc6dYKxY2HTTZOOSKRSZHJ5bElfhfat7EBEqrWvvoK99gonrHv0gEmT4Kyzko5KpFKkO0dxOqHl8H9mNjFlVhPgvbgDE6lWli0Ld1UPGQJ//Suo2rLkkHTnKJ4EXgVuBC5JeX2Ju38fa1Qi1cGkSeGGuSuuCFc0zZkT7rIWyTHpup7c3WcDZwJLUh6Y2cbxhyZSRf36K/zjH6GI3913FxXxU5KQHFVWi+IA4BPC5bGpbWkH/i/GuESqpg8/DAMKTZkCxx8fqr02b550VCKxKjVRuPsB0c+22QtHpApbtgz23z/UaHrlFdhX13RIzZBJrafdzKxR9Pw4M7vdzLaMPzSRKuJ//ysq4vfii6GIn5KE1CCZXB57P7DczLoCfwfmAI/HGpVIVfDjj2EY0p13Lirit+uu0KRJomGJZFsmiWK1uztwMHCXu99FuERWJHc9/3y4cW7o0FB644gjko5IJDGZVI9dYmaXAscDvcysFlAn3rBEEnT++eEkddeuoaupW7ekIxJJVCaJ4ijgGOCv7r4wOj9xa7xhiWRZahG//fYLVzL9/e9QR9+JRMrsenL3hcATQFMzOwBY4e6PxR6ZSLbMnRuuZrrqqjC9995w+eVKEiKRTK56OhL4CDiCMG72/8xMo65I9bd2Ldx3H3TuDG+9BZttlnREIlVSJl1PlwM7uvu3AGbWEngdGBlnYCKxmjkz1GR6551QAnzwYGjTJumoRKqkTBLFBoVJIrKYzK6WEqm6VqyAGTPg3/+Gv/xFRfxE0sgkUbxmZqMJ42ZDOLn9SnwhicRkwoRQxO+qq2DbbWH2bKhfP+moRKq8TE5mXwQ8CHQBugKD3f3iuAMTqTQrVoST0927w/33FxXxU5IQyUi68SjaA7cBfwQmARe6+9fZCkykUrz/fijiN21a6GK6/XbYWMWPRdZHuhbFI8BLwGGECrL3ZCUikcqybBkceCAsXw6vvRbuslaSEFlv6c5RNHH3h6Ln083s02wEJFJhH3wAO+0Uivi99FI4H6H6TCLllq5FUd/MtjezHcxsB6BBsekymVk/M5tuZjPN7JI0y+1oZmt0f4ZUyA8/hEted90VHo/qVu6yi5KESAWla1EsAG5PmV6YMu3AXulWHNWEGgTsAxQAH5vZKHefUsJyNwOj1y90kRTPPgtnngmLFsGll8JRRyUdkUjOSDdw0Z4VXHcPYKa7fwVgZiMIFWinFFvuLOA/wI4V3J7UVOedB3feCXl5YUCh7bdPOiKRnJLJfRTltTkwL2W6ANgpdQEz2xw4lNA6KTVRmNkpwCkAW26pMZOEdYv4HXAAbLIJXHih6jOJxCDOO6xLutXVi03fCVzs7mvSrcjdB7t7d3fv3rJly8qKT6qr2bOhXz+48sow3adP6G5SkhCJRZyJogDYImW6NTC/2DLdgRFmNhs4HLjPzA6JMSapztauhXvuCVcxvf8+bLVV0hGJ1Ahldj2ZmQHHAv/n7tdE41Fs6u4flfHWj4H2ZtYW+BroTxjX4jfu3jZlO0OBl9z9+fX6BFIzfPEFnHgivPdeaE088IAShUiWZNKiuA/YBTg6ml5CuJopLXdfDQwkXM00FXja3Seb2Wlmdlo545WaauVK+PJLeOyxcMJaSUIkazI5mb2Tu+9gZp8BuPsPZlY3k5W7+ysUKyDo7g+UsuwJmaxTapDPPgtF/K6+OowZMXs21KuXdFQiNU4mLYpV0b0ODr+NR7E21qikZluxIpyc3nFHePDBcG8EKEmIJCSTRHE38BywiZldD7wL3BBrVFJzvfsudO0KN90EAwbAlCmgK91EElVm15O7P2FmnwB9CJe8HuLuU2OPTGqepUvh4INhww1hzJgw8pyIJC6Tq562BJYDL6a+5u5z4wxMapB33w31mRo3hpdfDpe/Nm6cdFQiEsmk6+llQrnxl4E3gK+AV+MMSmqIxYtD91KvXkVF/HbeWUlCpIrJpOtpu9TpqHLsqbFFJLnPHUaOhIED4fvvwx3W/fsnHZWIlGK9az25+6dmpgJ+Un7nnQd33QXduoVzEV27Jh2RiKSRyTmK81MmNwB2ABbFFpHkJndqrV3Nmg3qwEEHwWabwfnnh6J+IlKlZXKOoknKox7hXMXBcQYlOWbWLOjbl5NmR0X89toL/v53JQmRaiLtf2p0o11jd78oS/FILlmzBu69Fy67DGrVYv6mRyQdkYiUQ6ktCjOrHZX/zmjYU5F1zJgRrmY691zYYw+YPJmXNjsl6ahEpBzStSg+IiSJCWY2CngGWFY4092fjTk2qc5Wr4Y5c2DYMDjmGLCShicRkeogk07ijYHFhFHonHB3tgNKFLKu8eNDEb9rr4VOneCrr1SfSSQHpEsUm0RXPOVTlCAKFR+pTmqyX36Bq66Cf/0LNt0Uzj471GdSkhDJCemueqoFNI4eTVKeFz5E4K23oEsXuPVWOOkkmDxZRfxEcky6FsUCd78ma5FI9bN0Kfz5z9CsGbzxRrjsVURyTrpEobOPUrJ33oHddgs1mV59NQwq1KhR0lGJSEzSdT31yVoUUj189x0cdxzsvntREb8ePZQkRHJcqS0Kd/8+m4FIFeYOTz8NZ50FP/wQTlyriJ9IjaEaClK2c86Be+4JQ5O+8QZst13Z7xGRnKFEISVzh1WroG5dOPRQ2GqrcJd1rVpJRyYiWZZJUUCpab78Evr0gSuuCNN77gkXXKAkIVJDKVFIkTVr4PbbQ9fSJ59Ahw5JRyQiVYC6niSYNg3+8hf46CM48EC4/37YfPOkoxKRKkCJQoK1a2H+fBg+HI46SkX8ROQ35l69yjY1adLdu3Ubn3QYOWGbnz9it8Uv8HDb6wGovXYlqzeoG9v2JkyAvDwYNy62TYhIKczsE3fvXp73VrtzFL/8knQE1V+9Ncs5/csLGfTZLvRb+ChNV4aRbeNMEhCSxDHHxLoJEYlBtet6atBA30gr5M034eSToeArOPVUWt58My80bZp0VCJShVW7RCEVsHQpHHFEKOL35pvQu3fSEYlINVDtup6kHMaNCyerC4v4TZyoJCEiGVOiyGWLFsHRR4cb5oYNC6/tuCM0bJhsXCJSrajrKRe5h8tczz4bliwJQ5OqiJ+IlJMSRS466ywYNAh23hkefjiMXy0iUk5KFLli7VpYvToU8Tv8cGjXLiQM1WcSkQqK9RyFmfUzs+lmNtPMLilh/rFmNjF6vG9mXeOMJ2d98UUYhvTyy8N0796q9CoilSa2RGFmtYBBwL5AJ+BoMyveBzIL2MPduwDXAoPjiicnrV4Nt90GXbqE2547dkw6IhHJQXF2PfUAZrr7VwBmNgI4GJhSuIC7v5+y/IdA6xjjyS1Tp8KAATB+PBx8MNx3H2y2WdJRiUgOirPraXNgXsp0QfRaaU4CXi1phpmdYmbjzWz8qlWrKjHEau6bb+Cpp+C555QkRCQ2cbYoSio/WmIFQjPbk5AoepY0390HE3VLNWnSvXpVMaxMH34IL7wAN94Yupm+/BLq1Ek6KhHJcXG2KAqALVKmWwPziy9kZl2AIcDB7r44xniqr2XL4LzzYNdd4Yknwo10oCQhIlkRZ6L4GGhvZm3NrC7QHxiVuoCZbQk8Cxzv7jNijKX6ev112HZbuPNOOOMMmDwZWrZMOioRqUFi63py99VmNhAYDdQCHnH3yWZ2WjT/AeAfQHPgPgsD5awub730nLR0abijeuON4e23oVevpCMSkRqoWg5ctGRJjg9cNHYs7LFHuA/ik0/CndUNGiQdlYhUYzVq4KKc9s03cOSR0KdPURG/bt2UJEQkUUoUVYE7PP54aDm88AJcf72GghORKkO1nqqCM8+E+++HXXYJRfx0h7WIVCFKFElZuxZWrYJ69eCoo0JyOOMM1WcSkSpHXU9JmD49nKwuLOK3xx6q9CoiVZYSRTatWgU33QRdu0J+Pmy3XdIRiYiUSV1P2TJ5Mhx/PHz2Gfz5z2FgoU03TToqEZEyKVFkS61a8P33MHIkHHZY0tGIiGRMXU9xev99uPji8HybbWDmTCUJEal2lCjisHQpnH029OwZyoB/9114vbYacCJS/ShRVLYxY0IRv3vvhYEDw0nrFi2SjkpEpNz0FbcyLV0Kxx4LzZvDO+/AbrslHZGISIWpRVEZ/vtfWLMGGjcOLYoJE5QkRCRnKFFUxIIF4eR0375hQCGA7beH+vWTjUtEpBIpUZSHOwwdGor4vfxyuIlORfxEJEfpHEV5nH46PPhguKppyBDo0CHpiESqpFWrVlFQUMCKFSuSDqXGqF+/Pq1bt6ZOJQ6VrESRqdQifsccA126wGmnwQZqlImUpqCggCZNmtCmTRuiUSwlRu7O4sWLKSgooG3btpW2Xh3lMjF1ahiG9LLLwvTuu4dKr0oSImmtWLGC5s2bK0lkiZnRvHnzSm/B6UiXzqpVcMMNkJcH06aFE9Uisl6UJLIrjv2trqfSTJ4Mxx0XLnU94gi45x5o1SrpqEREsk4titLUrg0//QTPPgtPP60kIVKNPffcc5gZ06ZN++21cePGccABB6yz3AknnMDIkSOBcCL+kksuoX379my77bb06NGDV199tcKx3HjjjbRr144OHTowevToEpc56qijyMvLIy8vjzZt2pCXlwfA4sWL2XPPPWncuDEDBw6scCyZUosi1TvvhDGrb7stXMk0Y4bqM4nkgOHDh9OzZ09GjBjB1VdfndF7rrzyShYsWEB+fj716tXjm2++4a233qpQHFOmTGHEiBFMnjyZ+fPns/feezNjxgxqFRu07Kmnnvrt+QUXXEDTpk2BcEXTtddeS35+Pvn5+RWKZX3oKAiwZAlccgncdx+0bRuet2ihJCFSic49N/TkVqa8PLjzzvTLLF26lPfee48333yTgw46KKNEsXz5ch566CFmzZpFvXr1AGjVqhVHHnlkheJ94YUX6N+/P/Xq1aNt27a0a9eOjz76iF122aXE5d2dp59+mrFjxwLQqFEjevbsycyZMysUx/pS19Orr0LnznD//eEvedIkFfETySHPP/88/fr1Y+utt2bjjTfm008/LfM9M2fOZMstt2TDDTcsc9nzzjvvt26i1MdNN930u2W//vprtthii9+mW7duzddff13qut955x1atWpF+/bty4wjTjX7K/OSJTBgAGyySRg7Yuedk45IJGeV9c0/LsOHD+fcc88FoH///gwfPpwddtih1KuD1veqoTvuuCPjZd19vbY3fPhwjj766PWKJw41L1G4w+jRsM8+0KQJvP56GFQoal6KSO5YvHgxY8eOJT8/HzNjzZo1mBm33HILzZs354cfflhn+e+//54WLVrQrl075s6dy5IlS2jSpEnabZx33nm8+eabv3u9f//+XHLJJeu81rp1a+bNm/fbdEFBAZtttlmJ6129ejXPPvssn3zySaYfNzY1q+tpwYIwXvW++xYV8evaVUlCJEeNHDmSAQMGMGfOHGbPns28efNo27Yt7777Lu3bt2f+/PlMnToVgDlz5vD555+Tl5dHw4YNOemkkzj77LNZuXIlAAsWLGDYsGG/28Ydd9zBhAkTfvconiQADjroIEaMGMGvv/7KrFmz+OKLL+jRo0eJsb/++utss802tG7duhL3SPnUjEThDo88Ah07wmuvwS23qIifSA0wfPhwDj300HVeO+yww3jyySepV68ew4YN48QTTyQvL4/DDz+cIUOG/HaF0XXXXUfLli3p1KkT2267LYcccggtW7asUDydO3fmyCOPpFOnTvTr149Bgwb9dsXTySefzPjx439bdsSIESV2O7Vp04bzzz+foUOH0rp1a6ZMmVKhmDJhJfWZVWVNmnT3JUvGl71gqlNPhcGDQ+mNIUMg4RNDIjXF1KlT6dixY9Jh1Dgl7Xcz+8Tdu5dnfbl7jmLNmlCCo379cIf19tvDKaeoPpOIyHrKzaPm5MlhhLnCIn69eqnSq4hIOeXWkXPlSrj22tB6mDkTdtwx6YhEarzq1r1d3cWxv3On62nSJDj22PCzf3+4+26o4IknEamY+vXrs3jxYpUaz5LC8SjqV/JwzLmTKOrWheXLQ62mgw5KOhoRIdw3UFBQwKJFi5IOpcYoHOGuMlXvRPHWWzBqFPzrX6GI3/TpUKy4logkp06dOpU60pokI9ZzFGbWz8ymm9lMM/vd3ScW3B3Nn2hmO2S04p9/DuNW9+4Nzz8P330XXleSEBGpdLElCjOrBQwC9gU6AUebWadii+0LtI8epwD3l7Xexqt/CkX8Bg+G889XET8RkZjF2aLoAcx096/cfSUwAji42DIHA4958CHQzMz+kG6lm/46G5o2DUX8/vUvaNgwluBFRCSI8xzF5sC8lOkCYKcMltkcWJC6kJmdQmhxAPxqkyfnq9IrAC2A75IOoorQviiifVFE+6JIh/K+Mc5EUdK1cMUv8M1kGdx9MDAYwMzGl/c29FyjfVFE+6KI9kUR7YsiZraetY+KxNn1VABskTLdGphfjmVERCRBcSaKj4H2ZtbWzOoC/YFRxZYZBQyIrn7aGfjJ3RcUX5GIiCQntq4nd19tZgOB0UAt4BF3n2xmp0XzHwBeAfYDZgLLgRMzWPXgmEKujrQvimhfFNG+KKJ9UaTc+6LalRkXEZHsyq2igCIiUumUKEREJK0qmyhiK/9RDWWwL46N9sFEM3vfzLomEWc2lLUvUpbb0czWmNnh2YwvmzLZF2bW28wmmNlkM3sr2zFmSwb/I03N7EUz+zzaF5mcD612zOwRM/vWzPJLmV++46a7V7kH4eT3l8D/AXWBz4FOxZbZD3iVcC/GzsD/ko47wX2xK7BR9HzfmrwvUpYbS7hY4vCk407w76IZMAXYMpreJOm4E9wXlwE3R89bAt8DdZOOPYZ9sTuwA5BfyvxyHTeraosilvIf1VSZ+8Ld33f3H6LJDwn3o+SiTP4uAM4C/gN8m83gsiyTfXEM8Ky7zwVw91zdH5nsCweaWBgUozEhUazObpjxc/e3CZ+tNOU6blbVRFFaaY/1XSYXrO/nPInwjSEXlbkvzGxz4FDggSzGlYRM/i62BjYys3Fm9omZDchadNmVyb64F+hIuKF3EnCOu6/NTnhVSrmOm1V1PIpKK/+RAzL+nGa2JyFR9Iw1ouRksi/uBC529zU5PqJaJvuiNtAN6AM0AD4wsw/dfUbcwWVZJvviT8AEYC/gj8B/zewdd/855tiqmnIdN6tqolD5jyIZfU4z6wIMAfZ198VZii3bMtkX3YERUZJoAexnZqvd/fmsRJg9mf6PfOfuy4BlZvY20BXItUSRyb44EbjJQ0f9TDObBWwDfJSdEKuMch03q2rXk8p/FClzX5jZlsCzwPE5+G0xVZn7wt3bunsbd28DjATOyMEkAZn9j7wA9DKz2mbWkFC9eWqW48yGTPbFXELLCjNrRaik+lVWo6waynXcrJItCo+v/Ee1k+G++AfQHLgv+ia92nOwYmaG+6JGyGRfuPtUM3sNmAisBYa4e4mXTVZnGf5dXAsMNbNJhO6Xi90958qPm9lwoDfQwswKgKuAOlCx46ZKeIiISFpVtetJRESqCCUKERFJS4lCRETSUqIQEZG0lChERCQtJQqpkqLKrxNSHm3SLLu0ErY31MxmRdv61Mx2Kcc6hphZp+j5ZcXmvV/RGKP1FO6X/KgaarMyls8zs/0qY9tSc+nyWKmSzGypuzeu7GXTrGMo8JK7jzSzvsBt7t6lAuurcExlrdfMHgVmuPv1aZY/Aeju7gMrOxapOdSikGrBzBqb2RvRt/1JZva7qrFm9gczezvlG3ev6PW+ZvZB9N5nzKysA/jbQLvovedH68o3s3Oj1xqZ2cvR2Ab5ZnZU9Po4M+tuZjcBDaI4nojmLY1+PpX6DT9qyRxmZrXM7FYz+9jCOAGnZrBbPiAq6GZmPSyMRfJZ9LNDdJfyNcBRUSxHRbE/Em3ns5L2o8jvJF0/XQ89SnoAawhF3CYAzxGqCGwYzWtBuLO0sEW8NPp5AXB59LwW0CRa9m2gUfT6xcA/StjeUKKxK4AjgP8RCupNAhoRSlNPBrYHDgMeSnlv0+jnOMK3999iSlmmMMZDgUej53UJlTwbAKcAV0Sv1wPGA21LiHNpyud7BugXTW8I1I6e7w38J3p+AnBvyvtvAI6Lnjcj1H1qlPTvW4+q/aiSJTxEgF/cPa9wwszqADeY2e6EchSbA62AhSnv+Rh4JFr2eXefYGZ7AJ2A96LyJnUJ38RLcquZXQEsIlTh7QM856GoHmb2LNALeA24zcxuJnRXvbMen+tV4G4zqwf0A95291+i7q4uVjQiX1OgPTCr2PsbmNkEoA3wCfDflOUfNbP2hGqgdUrZfl/gIDO7MJquD2xJbtaAkkqiRCHVxbGEkcm6ufsqM5tNOMj9xt3fjhLJ/sDjZnYr8APwX3c/OoNtXOTuIwsnzGzvkhZy9xlm1o1QM+dGMxvj7tdk8iHcfYWZjSOUvT4KGF64OeAsdx9dxip+cfc8M2sKvAScCdxNqGX0prsfGp34H1fK+w04zN2nZxKvCOgchVQfTYFvoySxJ7BV8QXMbKtomYeAhwlDQn4I7GZmheccGprZ1hlu823gkOg9jQjdRu+Y2WbAcncfBtwWbae4VVHLpiQjCMXYehEK2RH9PL3wPWa2dbTNErn7T8DZwIXRe5oCX0ezT0hZdAmhC67QaOAsi5pXZrZ9adsQKaREIdXFE0B3MxtPaF1MK2GZ3sAEM/uMcB7hLndfRDhwDjeziYTEsU0mG3T3TwnnLj4inLMY4u6fAdsBH0VdQJcD15Xw9sHAxMKT2cWMIYxt/LqHoTshjCUyBfjUzPKBBymjxR/F8jmhrPYthNbNe4TzF4XeBDoVnswmtDzqRLHlR9MiaenyWBERSUstChERSUuJQkRE0lKiEBGRtJQoREQkLSUKERFJS4lCRETSUqIQEZG0/h++gk/RnKI2FwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ebfb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.86 s, sys: 516 ms, total: 3.38 s\n",
      "Wall time: 641 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8384e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews predicted positive:  0\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the probabilities\n",
    "threshold = 0.63\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of reviews predicted positive: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50dc4d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds\n",
    "\n",
    "y_hat = []\n",
    "\n",
    "for i in preds:\n",
    "    if i:\n",
    "        y_hat.append('positive')\n",
    "    else: y_hat.append('negative')\n",
    "\n",
    "y_hat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f22669c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= 5000\n",
    "b= a + 20\n",
    "[i for i in zip(y_hat[a:b], X_test[a:b])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9f2d31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>style</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>10 24, 2017</td>\n",
       "      <td>A2HAJB8L9NVYTZ</td>\n",
       "      <td>B007Y1AMHE</td>\n",
       "      <td>ok</td>\n",
       "      <td>ok</td>\n",
       "      <td>1508803200</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>04 8, 2015</td>\n",
       "      <td>AD78RH9JWBDEU</td>\n",
       "      <td>B007Y1AMHE</td>\n",
       "      <td>Its awesome</td>\n",
       "      <td>love it</td>\n",
       "      <td>1428451200</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verified   reviewTime      reviewerID        asin   reviewText  summary  \\\n",
       "0      True  10 24, 2017  A2HAJB8L9NVYTZ  B007Y1AMHE           ok       ok   \n",
       "1      True   04 8, 2015   AD78RH9JWBDEU  B007Y1AMHE  Its awesome  love it   \n",
       "\n",
       "   unixReviewTime sentiment  id style  vote image  \n",
       "0      1508803200  negative   0   NaN   NaN   NaN  \n",
       "1      1428451200  negative   1   NaN   NaN   NaN  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_pred = pd.read_json( '../data/raw/music_reviews_test_masked.json.gz', lines=True)\n",
    "test_pred = test_pred[0:10]\n",
    "\n",
    "test_pred['sentiment'] = y_hat\n",
    "\n",
    "test_pred.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.to_json('../data/predictions/music_reviews_test'+current_time+'.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7a1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a38f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ff183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
