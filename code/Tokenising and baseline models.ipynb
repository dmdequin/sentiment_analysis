{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a105f6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f7b379ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d9438dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_29032022_144252\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"_%d%m%Y_%H%M%S\")\n",
    "print('this'+current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ff764",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e6981440",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/interim/train.csv'\n",
    "DEV   = '../data/interim/dev.csv'\n",
    "TEST  = '../data/interim/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a12e91",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b0a97f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(PATH):\n",
    "    with open(PATH, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        text = []\n",
    "        for lines in csvFile:\n",
    "            text.append(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def splitter(L):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in L:\n",
    "        X.append(i[0])\n",
    "        y.append(int(i[1]))\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ebe51fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tknzr(sentence):\n",
    "    \"\"\"Function to find all tokens in a given sentence\n",
    "    \"\"\"\n",
    "    tok = re.compile('[\\'\\\"]|[A-Za-z]+|[.?!:\\'\\\"]+')\n",
    "    \n",
    "    return tok.findall(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee794c10",
   "metadata": {},
   "source": [
    "# Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4120bc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140634c",
   "metadata": {},
   "source": [
    "# Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bf9ceb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06495b8f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "43eb346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loader(TRAIN) # Training\n",
    "dev_data = loader(DEV)     # Validation\n",
    "X_test = loader(TEST)      # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "22c526e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10000, 10000)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "37260ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to select a subset of the data\n",
    "train_data = train_data[0:50] \n",
    "dev_data = dev_data[0:10]\n",
    "X_test = X_test[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "870f2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!',\n",
       " '1']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad3020",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ea7ecd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = splitter(train_data)\n",
    "X_dev, y_dev = splitter(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8e915d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gotta', 'listen', 'to', 'this', '!', 'So', 'creative', '!', 'Love', 'his', 'music', 'the', 'words', 'the', 'message', '!', 'Some', 'of', 'my', 'favorite', 'songs', 'on', 'this', 'CD', '.', 'I', 'should', 'have', 'bought', 'it', 'years', 'ago', '!']\n"
     ]
    }
   ],
   "source": [
    "# hand-made tokenization\n",
    "print(tknzr(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "89f2a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gotta',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'this',\n",
       " '!',\n",
       " 'So',\n",
       " 'creative',\n",
       " '!',\n",
       " 'Love',\n",
       " 'his',\n",
       " 'music',\n",
       " 'the',\n",
       " 'words',\n",
       " 'the',\n",
       " 'message',\n",
       " '!',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'songs',\n",
       " 'on',\n",
       " 'this',\n",
       " 'CD',\n",
       " '.',\n",
       " 'I',\n",
       " 'should',\n",
       " 'have',\n",
       " 'bought',\n",
       " 'it',\n",
       " 'years',\n",
       " 'ago',\n",
       " '!']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X_train_tokens = []\n",
    "for sentence in X_train:\n",
    "    temp = tknzr(sentence)\n",
    "    if len(temp) > 0:\n",
    "        if len(temp) > 500:\n",
    "            X_train_tokens.append(temp[0:500])\n",
    "        else: X_train_tokens.append(temp)\n",
    "    else: X_train_tokens.append('NULL')\n",
    "print(len(X_train_tokens))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5124a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for x in X_train_tokens:\n",
    "    if len(x) == 0 or len(x) > 500:\n",
    "        print(x)\n",
    "        \n",
    "for i in X_train_tokens:\n",
    "    if i == []:\n",
    "        print('!!!')\"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "fbf56264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"X_dev_tokens = []\n",
    "for sentence in X_dev:\n",
    "    X_dev_tokens.append(tknzr(sentence))\n",
    "print(len(X_dev_tokens))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "647f43bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ok ok'],\n",
       " 'Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b4ac281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "\"\"\"X_test_tokens = []\n",
    "for sentence in X_test:\n",
    "    X_test_tokens.append(tknzr(str(sentence)))\n",
    "print(len(X_test_tokens))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e25e321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,      # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,           # Max length to truncate/pad\n",
    "            padding='max_length',         # Pad sentence to max length\n",
    "            #return_tensors='pt',         # Return PyTorch tensor\n",
    "            return_attention_mask=True,   # Return attention mask\n",
    "            truncation = True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids')[:512])\n",
    "        attention_masks.append(encoded_sent.get('attention_mask')[:512])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "48b06be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 10657, 4952, 2000, 2023, 999, 2061, 5541, 999, 2293, 2010, 2189, 1011, 1996, 2616, 1010, 1996, 4471, 999, 2070, 1997, 2026, 5440, 2774, 2006, 2023, 3729, 1012, 1045, 2323, 2031, 4149, 2009, 2086, 3283, 999, 102], [101, 9467, 9467, 2023, 6823, 2064, 6684, 2022, 5319, 1998, 2009, 2001, 3205, 2005, 5096, 2004, 1000, 2200, 2204, 1000, 1012, 2009, 1005, 1055, 2200, 2919, 1012, 102]]\n"
     ]
    }
   ],
   "source": [
    "# Encode our concatenated data\n",
    "encoded_ = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]\n",
    "print(encoded_[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "65aebe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for sent in encoded_:\n",
    "    if len(sent) > l:\n",
    "        l = len(sent)\n",
    "        \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c67030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!\n",
      "Token IDs:  tensor([[  101, 10657,  4952,  ...,     0,     0,     0],\n",
      "        [  101,  9467,  9467,  ...,     0,     0,     0]])\n",
      "Tokenizing data...\n",
      "F'ing Done!!\n"
     ]
    }
   ],
   "source": [
    "# THIS IS JUST A TEST\n",
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = l\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = preprocessing_for_bert(X_train[0:2])\n",
    "print('Original: ', X_train[0])\n",
    "print('Token IDs: ', token_ids[0])\n",
    "\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_dev)\n",
    "print('F\\'ing Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1be07fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 10, 10)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(train_masks), len(val_inputs), len(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "fb63408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3e555c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_dev)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fafd2",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "792aed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 74.6 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size, max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a605a",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dc2f3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "492644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into evaluation mode. Dropout layers are disabled during test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4cb85",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "13544696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "   1    |   20    |   0.660712   |     -      |     -     |  166.17  \n",
      "   1    |   24    |   0.530673   |     -      |     -     |   24.58  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.639906   |  0.604700  |   60.00   |  197.42  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "   2    |   20    |   0.295443   |     -      |     -     |  395.84  \n",
      "   2    |   24    |   0.132450   |     -      |     -     |   26.33  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.269364   |  0.393815  |   90.00   |  428.03  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "CPU times: user 20min 25s, sys: 11min 15s, total: 31min 40s\n",
      "Wall time: 10min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945586e",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ea574235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model = bert_classifier\n",
    "pickle.dump(model, open('model' + current_time + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5667eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = pickle.load(open('model' + current_time + '.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f676fb6",
   "metadata": {},
   "source": [
    "# Run Preprocessing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a1d600e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and embed data...\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing and embed data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(X_test_tokens)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6189151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into evaluation mode. Dropout layers are disabled during testing.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "71748c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdcf25",
   "metadata": {},
   "source": [
    "# Predict Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f2f1db9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.2 s, sys: 2.44 s, total: 35.7 s\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute predicted probabilities on the validation set\n",
    "probs = bert_predict(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6acb3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews predicted positive:  2\n",
      "Accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the probabilities\n",
    "threshold = 0.63\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of reviews predicted positive: \", preds.sum())\n",
    "\n",
    "print(f'Accuracy: {round(100 * sum(preds==y_dev)/len(y_dev), 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0d22919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9167\n",
      "Accuracy: 90.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6ebfb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 2min 49s, total: 5min 5s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b8384e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews predicted positive:  13\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the probabilities\n",
    "threshold = 0.63\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of reviews predicted positive: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "50dc4d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds\n",
    "\n",
    "y_hat = []\n",
    "\n",
    "for i in preds:\n",
    "    if i:\n",
    "        y_hat.append('positive')\n",
    "    else: y_hat.append('negative')\n",
    "\n",
    "y_hat[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f22669c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= 5000\n",
    "b= a + 20\n",
    "[i for i in zip(y_hat[a:b], X_test[a:b])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d9f2d31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>style</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>10 24, 2017</td>\n",
       "      <td>A2HAJB8L9NVYTZ</td>\n",
       "      <td>B007Y1AMHE</td>\n",
       "      <td>ok</td>\n",
       "      <td>ok</td>\n",
       "      <td>1508803200</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>04 8, 2015</td>\n",
       "      <td>AD78RH9JWBDEU</td>\n",
       "      <td>B007Y1AMHE</td>\n",
       "      <td>Its awesome</td>\n",
       "      <td>love it</td>\n",
       "      <td>1428451200</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verified   reviewTime      reviewerID        asin   reviewText  summary  \\\n",
       "0      True  10 24, 2017  A2HAJB8L9NVYTZ  B007Y1AMHE           ok       ok   \n",
       "1      True   04 8, 2015   AD78RH9JWBDEU  B007Y1AMHE  Its awesome  love it   \n",
       "\n",
       "   unixReviewTime sentiment  id style  vote image  \n",
       "0      1508803200  negative   0   NaN   NaN   NaN  \n",
       "1      1428451200  positive   1   NaN   NaN   NaN  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_pred = pd.read_json( '../data/raw/music_reviews_test_masked.json.gz', lines=True)\n",
    "test_pred = test_pred[0:50]\n",
    "\n",
    "test_pred['sentiment'] = y_hat\n",
    "\n",
    "test_pred.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "884c1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.to_json('../data/predictions/music_reviews_test'+current_time+'.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7a1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a38f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ff183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
