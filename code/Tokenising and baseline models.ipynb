{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a105f6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f7b379ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ff764",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e6981440",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/interim/train.csv'\n",
    "DEV   = '../data/interim/dev.csv'\n",
    "TEST  = '../data/interim/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a12e91",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b0a97f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(PATH):\n",
    "    with open(PATH, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        text = []\n",
    "        for lines in csvFile:\n",
    "            text.append(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def splitter(L):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in L:\n",
    "        X.append(i[0])\n",
    "        y.append(int(i[1]))\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06495b8f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "43eb346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loader(TRAIN) # Training\n",
    "dev_data = loader(DEV)     # Validation\n",
    "X_test = loader(TEST)      # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "22c526e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10000, 10000)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "37260ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to select a subset of the data\n",
    "train_data = train_data[0:5] \n",
    "dev_data = dev_data[0:10]\n",
    "X_test = X_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "870f2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!',\n",
       " '1']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad3020",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ebe51fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tknzr(sentence):\n",
    "    \"\"\"Function to find all tokens in a given sentence\n",
    "    \"\"\"\n",
    "    tok = re.compile('[\\'\\\"]|[A-Za-z]+|[.?!:\\'\\\"]+')\n",
    "    \n",
    "    return tok.findall(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ea7ecd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = splitter(train_data)\n",
    "X_dev, y_dev = splitter(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e915d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gotta', 'listen', 'to', 'this', '!', 'So', 'creative', '!', 'Love', 'his', 'music', 'the', 'words', 'the', 'message', '!', 'Some', 'of', 'my', 'favorite', 'songs', 'on', 'this', 'CD', '.', 'I', 'should', 'have', 'bought', 'it', 'years', 'ago', '!']\n"
     ]
    }
   ],
   "source": [
    "# hand-made tokenization\n",
    "print(tknzr(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "89f2a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "X_train_tokens = []\n",
    "for sentence in X_train:\n",
    "    temp = tknzr(sentence)\n",
    "    if len(temp) > 0:\n",
    "        if len(temp) > 500:\n",
    "            X_train_tokens.append(temp[0:500])\n",
    "        else: X_train_tokens.append(temp)\n",
    "    else: X_train_tokens.append('NULL')\n",
    "print(len(X_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5124a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train_tokens:\n",
    "    if len(x) == 0 or len(x) > 500:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3b76d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_train_tokens:\n",
    "    if i == []:\n",
    "        print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fbf56264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "X_dev_tokens = []\n",
    "for sentence in X_dev:\n",
    "    X_dev_tokens.append(tknzr(sentence))\n",
    "print(len(X_dev_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "647f43bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ok ok'],\n",
       " 'Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b4ac281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "X_test_tokens = []\n",
    "for sentence in X_test:\n",
    "    X_test_tokens.append(tknzr(str(sentence)))\n",
    "print(len(X_test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee794c10",
   "metadata": {},
   "source": [
    "# Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4120bc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e25e321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            padding='max_length',         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation = True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "48b06be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode our concatenated data\n",
    "encoded_ = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "65aebe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for sent in encoded_:\n",
    "    if len(sent) > l:\n",
    "        l = len(sent)\n",
    "        \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c67030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Gotta listen to this! So creative!  Love his music - the words, the message! Some of my favorite songs on this CD. I should have bought it years ago!\n",
      "Token IDs:  tensor([[  101, 10657,  4952,  2000,  2023,   999,  2061,  5541,   999,  2293,\n",
      "          2010,  2189,  1011,  1996,  2616,  1010,  1996,  4471,   999,  2070,\n",
      "          1997,  2026,  5440,  2774,  2006,  2023,  3729,  1012,  1045,  2323,\n",
      "          2031,  4149,  2009,  2086,  3283,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  9467,  9467,  2023,  6823,  2064,  6684,  2022,  5319,  1998,\n",
      "          2009,  2001,  3205,  2005,  5096,  2004,  1000,  2200,  2204,  1000,\n",
      "          1012,  2009,  1005,  1055,  2200,  2919,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "Tokenizing data...\n",
      "F'ing Done!!\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = l\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = preprocessing_for_bert(X_train[0:2])\n",
    "print('Original: ', X_train[0])\n",
    "print('Token IDs: ', token_ids[0])\n",
    "\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_dev)\n",
    "print('F\\'ing Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1be07fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 10, 10)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(train_masks), len(val_inputs), len(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fb63408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 10657,  4952,  2000,  2023,   999,  2061,  5541,   999,  2293,\n",
       "          2010,  2189,  1011,  1996,  2616,  1010,  1996,  4471,   999,  2070,\n",
       "          1997,  2026,  5440,  2774,  2006,  2023,  3729,  1012,  1045,  2323,\n",
       "          2031,  4149,  2009,  2086,  3283,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  9467,  9467,  2023,  6823,  2064,  6684,  2022,  5319,  1998,\n",
       "          2009,  2001,  3205,  2005,  5096,  2004,  1000,  2200,  2204,  1000,\n",
       "          1012,  2009,  1005,  1055,  2200,  2919,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  4965,  1996,  3729,  1012,  2079,  2025,  4965,  1996, 23378,\n",
       "          1012,  4965,  1996,  3729,  1012,  2079,  2025,  4965,  1996, 23378,\n",
       "          2201,  1012,  8816,  2003,  2053,  2936,  2800,  1012,  2021,  2017,\n",
       "          2123,  1005,  1056,  2424,  2008,  2041,  2127,  2044,  2017,  2031,\n",
       "          4156,  2009,  1012,   102],\n",
       "        [  101,  2274,  3340,  1045,  2293,  5759, 28925,  2015,  2189,  1998,\n",
       "          2376,   999,  4067,  2017,   999,  1045,  2097,  2022,  7052,  2035,\n",
       "          2010,  6759,  1999,  6014,  1010,  5091,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2274,  3340,  2307,  5758,  1997,  2026,  2220,  2086,  1999,\n",
       "          4828,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3e555c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_dev)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fafd2",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "792aed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 81.8 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size, max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a605a",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dc2f3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "492644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4cb85",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "13544696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/dmdequin/miniconda3/envs/uni/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |    0    |   0.671327   |     -      |     -     |   2.08   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.671327   |  0.703941  |   40.00   |   2.49   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |    0    |   0.571595   |     -      |     -     |   1.65   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.571595   |  0.696200  |   50.00   |   2.06   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "CPU times: user 19.6 s, sys: 5.45 s, total: 25.1 s\n",
      "Wall time: 9.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945586e",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ea574235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model = bert_classifier\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5667eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f676fb6",
   "metadata": {},
   "source": [
    "# Run Preprocessing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a1d600e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and embed data...\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing and embed data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(X_test_tokens)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6189151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "71748c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdcf25",
   "metadata": {},
   "source": [
    "# Predict Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f2f1db9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.55 s, sys: 344 ms, total: 2.89 s\n",
      "Wall time: 604 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute predicted probabilities on the validation set\n",
    "probs = bert_predict(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6acb3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews predicted positive:  0\n",
      "Accuracy: 60.0%\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the probabilities\n",
    "threshold = 0.63\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of reviews predicted positive: \", preds.sum())\n",
    "\n",
    "print(f'Accuracy: {round(100 * sum(preds==y_dev)/len(y_dev), 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0d22919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6667\n",
      "Accuracy: 50.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6ebfb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.8 s, sys: 266 ms, total: 3.06 s\n",
      "Wall time: 652 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b8384e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews predicted positive:  0\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the probabilities\n",
    "threshold = 0.63\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of reviews predicted positive: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "50dc4d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds\n",
    "\n",
    "y_hat = []\n",
    "\n",
    "for i in preds:\n",
    "    if i:\n",
    "        y_hat.append('positive')\n",
    "    else: y_hat.append('negative')\n",
    "\n",
    "y_hat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f22669c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= 5000\n",
    "b= a + 20\n",
    "[i for i in zip(y_hat[a:b], X_test[a:b])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d9f2d31a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (10) does not match length of index (10000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_174/3357935896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'../data/raw/music_reviews_test_masked.json.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3653\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3830\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m         \"\"\"\n\u001b[0;32m-> 3832\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3834\u001b[0m         if (\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4529\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4530\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \"\"\"\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (10) does not match length of index (10000)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_pred = pd.read_json( '../data/raw/music_reviews_test_masked.json.gz', lines=True)\n",
    "\n",
    "test_pred['sentiment'] = y_hat\n",
    "\n",
    "test_pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.to_json('../data/predictions/music_reviews_test.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7a1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a38f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ff183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
