{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(json_file):\n",
    "    df = pd.read_json(json_file, orient='records', lines=True)\n",
    "\n",
    "    # remove 3 reviews\n",
    "    df_no3 = df[df['overall'].isin([1,2,4,5])]\n",
    "\n",
    "    dict_class = {\n",
    "    1 : 0,\n",
    "    2 : 0,\n",
    "    4 : 1,\n",
    "    5 : 1\n",
    "    }\n",
    "\n",
    "    # map reviews to sentiment classification\n",
    "    df_no3['sentiment'] = df_no3['overall'].map(dict_class)\n",
    "    df_no3['rev_sum'] = df_no3['summary'] + ' ' + df_no3['reviewText']\n",
    "    # df_no3.head(3)\n",
    "\n",
    "    # get only relevant columns\n",
    "    df_games = pd.DataFrame()\n",
    "    return df_no3[['rev_sum', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None) # turn off warning\n",
    "\n",
    "df_vg = pre_process('Video_Games_5.json')\n",
    "df_dm = pre_process('Digital_Music_5.json')\n",
    "df_ac = pre_process('Arts_Crafts_and_Sewing_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vg_joint = ' '.join(df_vg['rev_sum'].astype(str))\n",
    "# dm_joint = ' '.join(df_dm['rev_sum'].astype(str))\n",
    "# ac_joint = ' '.join(df_ac['rev_sum'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_text(corpus1, corpus2):\n",
    "    \n",
    "    # tokenize corpus into sentences\n",
    "    # list of strings\n",
    "    first = sent_tokenize(corpus1)\n",
    "    second = sent_tokenize(corpus2)\n",
    "\n",
    "    # list of lists, words as tokens for corpus 1\n",
    "    gen_docs = [[w.lower() for w in word_tokenize(text)] for text in first]\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "    # # building the index\n",
    "    sims = gensim.similarities.Similarity('workdir/', tf_idf[corpus], num_features=len(dictionary))\n",
    "\n",
    "    for line in second:\n",
    "        # list of lists, words as tokens for corpus 2\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "\n",
    "        # update an existing dictionary and create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "    sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))/len(first)\n",
    "    print(sum_of_sims)\n",
    "\n",
    "    print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_vg.head(3)\n",
    "test['rev_sum'] = test['rev_sum'].apply(word_tokenize)\n",
    "test_list = list(test['rev_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_text2(df1, df2):\n",
    "    \n",
    "    # tokenize corpus into sentences\n",
    "    # list of strings\n",
    "    df1['rev_sum'] = df1['rev_sum'].astype(str).apply(word_tokenize)\n",
    "    df2['rev_sum'] = df2['rev_sum'].astype(str).apply(word_tokenize)\n",
    "\n",
    "    # list of lists, words as tokens for corpus 1\n",
    "    gen_docs = list(df1['rev_sum'])[:100]\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "    # # building the index\n",
    "    sims = gensim.similarities.Similarity('workdir/', tf_idf[corpus], num_features=len(dictionary))\n",
    "\n",
    "    # list of lists, words as tokens for corpus 2\n",
    "    query_doc = list(df2['rev_sum'])[:100]\n",
    "\n",
    "    # update an existing dictionary and create bag of words\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "    sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))/len(first)\n",
    "    print(sum_of_sims)\n",
    "\n",
    "    print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_text2(df_dm, df_vg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "\n",
    "def tok(x):\n",
    "    x = remove_stopwords(x.lower())\n",
    "    x = word_tokenize(x)\n",
    "    return x\n",
    "\n",
    "# s = \"This si sentence. ldksajdlksa, so cool, omg, bla bla!\"\n",
    "# tok(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, df2 = df_dm.head(2), df_dm.head(2)\n",
    "\n",
    "# tokenize corpus into sentences\n",
    "# list of strings\n",
    "df1['rev_sum'] = df1['rev_sum'].astype(str).apply(tok)\n",
    "df2['rev_sum'] = df2['rev_sum'].astype(str).apply(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists, words as tokens for corpus 1\n",
    "gen_docs = list(df1['rev_sum'])[:100]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # building the index\n",
    "sims = gensim.similarities.Similarity('workdir/', tf_idf[corpus], num_features=len(dictionary))\n",
    "\n",
    "# list of lists, words as tokens for corpus 2\n",
    "query_doc = list(df2['rev_sum'])[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6077032089233398\n",
      "Comparing Result: [0.9692234  0.24618298]\n"
     ]
    }
   ],
   "source": [
    "li = list()\n",
    "for i in query_doc:\n",
    "    for j in i:\n",
    "        li.append(j)\n",
    "\n",
    "# update an existing dictionary and create bag of words\n",
    "query_doc_bow = dictionary.doc2bow(li)\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))/len(gen_docs)\n",
    "print(sum_of_sims)\n",
    "\n",
    "print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62a3362af9fb780a0eb03402584da787fc1a1b0aa4d8f94c680e96f1c63d9193"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
