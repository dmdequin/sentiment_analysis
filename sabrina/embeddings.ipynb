{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f224b65df70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabrina/miniconda3/envs/uni/lib/python3.9/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /tmp/pip-req-build-19kunu9c/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1573,  1.0756, -0.9489, -0.7365,  0.2318, -0.1287, -0.6460,  0.3499,\n",
      "        -0.0975,  1.3571], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(ngrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "# print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 8.6918e-01,  2.5017e-01, -6.7802e-01,  8.3246e-01,  4.3768e-01,\n",
       "          1.1959e+00,  1.7906e+00, -1.1452e-01,  5.7259e-02, -6.3219e-01],\n",
       "        [-8.1768e-01, -1.4198e-01, -8.2205e-01,  3.7005e-01,  2.7966e-01,\n",
       "          1.7438e+00,  1.9428e-01,  2.4313e-02,  1.6510e-01, -4.8417e-01],\n",
       "        [-1.8855e+00, -7.8315e-01,  2.0533e+00, -7.0135e-02,  2.3641e+00,\n",
       "         -1.0346e+00,  1.5909e+00, -6.4176e-01,  2.4202e+00,  2.8292e-01],\n",
       "        [ 2.5861e-01,  1.2019e+00,  7.6570e-01,  4.3868e-01,  1.1683e+00,\n",
       "          2.0321e+00,  2.1061e-01, -5.2120e-01, -1.8174e+00, -1.3189e+00],\n",
       "        [ 1.5500e+00, -8.7698e-01, -3.3580e+00, -7.6113e-01,  1.1206e+00,\n",
       "          2.9694e-01,  1.1333e-01, -6.5103e-01, -2.0966e-01, -1.9840e-02],\n",
       "        [ 1.6875e+00,  4.5070e-03,  9.8505e-01,  8.8559e-01, -1.4577e+00,\n",
       "         -1.1946e+00, -4.6885e-01, -5.5854e-01,  4.1063e-01, -9.9577e-01],\n",
       "        [ 1.3452e+00,  8.5030e-01, -2.8079e-01,  7.0561e-01, -1.4465e+00,\n",
       "         -5.0426e-01,  1.6948e-01, -1.6521e-01, -5.0440e-01, -1.4970e+00],\n",
       "        [-3.3775e-01, -3.1446e-01, -1.0551e+00, -4.6935e-01,  2.9561e-01,\n",
       "          2.0092e+00, -9.0894e-01, -9.2907e-01,  1.4431e+00,  4.2517e-01],\n",
       "        [ 1.3915e+00, -8.3458e-01, -9.3889e-01,  1.1093e+00,  6.1260e-01,\n",
       "          1.9620e-01, -2.6153e+00,  8.5858e-01, -2.1286e+00, -6.1365e-01],\n",
       "        [-1.4868e+00, -1.1467e+00, -1.0439e-01,  3.4017e-01, -1.0671e+00,\n",
       "         -8.7527e-01,  1.9531e-01,  3.1101e-01,  5.3720e-01, -4.3878e-01],\n",
       "        [ 1.5600e-01,  1.2630e+00,  1.3501e+00, -5.1391e-01, -2.0068e+00,\n",
       "          1.8128e+00,  1.1358e-01,  3.4395e-01, -6.5796e-01, -2.9302e-01],\n",
       "        [ 3.3823e+00, -4.1558e-01, -2.8398e-01, -1.7972e+00,  6.3591e-01,\n",
       "         -8.1337e-01, -1.0570e+00, -1.0750e+00, -2.2509e-01, -2.0404e-01],\n",
       "        [ 2.3057e+00,  6.6571e-01,  1.7366e+00, -1.8212e+00, -1.5451e+00,\n",
       "          9.1816e-01, -5.5418e-01, -3.4788e-01,  4.6330e-01, -4.2461e-01],\n",
       "        [ 5.7003e-01, -1.5568e+00,  7.6611e-01, -4.1839e-01, -1.2343e-01,\n",
       "          2.7834e-01,  1.7567e+00,  1.8747e+00, -7.2458e-01,  2.5758e+00],\n",
       "        [ 8.1271e-01, -1.1105e+00, -2.0070e-01, -5.5844e-01, -6.3861e-01,\n",
       "         -1.0093e+00,  9.2152e-01,  9.9383e-01,  2.5750e-01, -7.9436e-01],\n",
       "        [ 5.3348e-01,  1.2424e+00, -3.8892e-01, -9.7424e-01, -8.0320e-03,\n",
       "         -7.3587e-02, -4.1386e-01, -7.1019e-02,  7.6302e-01, -1.0059e+00],\n",
       "        [-9.8657e-01,  1.4121e-01, -1.7983e-01, -3.5544e-01, -6.4731e-02,\n",
       "         -2.5147e+00,  2.3702e-01,  2.9221e-01,  1.0977e-01,  1.1023e+00],\n",
       "        [-3.2021e-01,  9.5591e-01,  6.4360e-01, -4.6318e-01, -2.7286e-01,\n",
       "          3.8368e-01,  5.0342e-01, -1.8590e+00, -2.9474e+00, -5.5412e-01],\n",
       "        [-1.2067e+00, -3.2612e-02, -2.0208e+00, -4.8106e-01, -1.5458e+00,\n",
       "         -1.8360e-01,  7.3390e-01,  5.9054e-02, -7.1854e-01, -5.2084e-01],\n",
       "        [-1.0925e+00, -7.1595e-01,  2.3309e-01,  1.9280e+00,  1.8627e+00,\n",
       "          1.3281e+00, -4.9345e-02,  3.3725e-01, -6.4647e-01,  1.5655e+00],\n",
       "        [ 2.1416e-01, -5.8412e-01,  1.4714e+00,  1.7769e+00, -2.0396e+00,\n",
       "          4.1440e-01,  5.7500e-01, -1.8095e+00,  1.2585e+00,  1.2920e+00],\n",
       "        [ 3.9815e-01,  3.8337e-01,  2.6725e+00, -9.6171e-01, -2.2084e-01,\n",
       "         -1.3892e+00,  1.7363e+00, -2.3718e+00, -9.3832e-01,  3.0175e-01],\n",
       "        [ 1.6641e+00,  2.6975e-01, -7.9537e-01,  1.2874e-01, -7.4497e-01,\n",
       "         -2.0982e-01, -2.3200e-01, -7.7227e-05,  1.5573e-01,  7.8194e-01],\n",
       "        [-2.3644e+00, -8.4834e-01, -1.0796e-01, -1.1846e+00, -9.2924e-02,\n",
       "          5.6738e-01, -5.9814e-01, -2.0859e+00,  4.4462e-01,  3.2940e-01],\n",
       "        [ 2.4384e+00,  1.0285e+00, -4.3178e-01, -1.7560e+00, -1.0181e+00,\n",
       "          5.3236e-01, -4.4907e-01, -1.2167e-01, -5.8622e-01,  2.1305e+00],\n",
       "        [-5.5359e-01, -2.4737e+00, -9.6079e-01, -3.1025e-01,  1.6818e+00,\n",
       "          4.9095e-01, -1.9607e-01, -3.7193e-02,  7.3613e-01,  1.0696e+00],\n",
       "        [-8.5350e-01, -1.2110e+00, -2.3161e+00,  6.0789e-01,  3.2804e-01,\n",
       "          1.1722e+00,  3.0432e-01, -5.7765e-01,  5.6779e-01, -9.0085e-01],\n",
       "        [-2.7764e-01,  1.3241e+00, -1.5680e+00, -1.0838e+00, -8.7200e-01,\n",
       "         -4.7162e-01, -7.5836e-01, -4.3849e-01,  6.7997e-01,  6.6137e-01],\n",
       "        [ 7.9248e-01,  1.3811e+00, -8.6691e-01,  5.1559e-01,  3.3204e-01,\n",
       "          1.1238e+00,  1.8318e-01, -2.6614e-01, -6.2361e-01,  8.1875e-01],\n",
       "        [ 2.5316e-01, -4.0111e-01, -2.0124e-01, -8.5115e-01, -1.5617e+00,\n",
       "         -5.4808e-02,  1.2034e+00, -2.2968e+00, -1.0197e+00, -9.0360e-01],\n",
       "        [ 1.4073e-01,  2.0595e-01,  1.0607e+00,  1.3779e-01,  1.4152e+00,\n",
       "          4.9594e-01, -6.9002e-01, -8.3907e-01,  1.0396e-01,  1.8576e-01],\n",
       "        [-1.2022e+00, -1.4533e+00, -7.1525e-01,  7.5980e-01, -1.1109e-02,\n",
       "          1.1155e+00,  5.2236e-01,  9.9013e-01, -1.2021e+00, -4.9597e-01],\n",
       "        [ 7.2044e-01,  1.5936e-01,  1.4813e-01, -2.3662e+00, -4.6700e-01,\n",
       "         -3.8564e-01,  3.4887e-01,  4.8927e-01,  1.9840e-01,  5.4820e-01],\n",
       "        [-4.0102e-01, -1.9337e+00,  9.4382e-01,  9.2185e-01, -7.8191e-01,\n",
       "         -5.7419e-01, -7.9111e-01, -1.4344e+00,  1.6222e+00, -4.5767e-02],\n",
       "        [-4.4201e-01, -1.1095e-01,  1.8130e+00, -4.8827e-01, -2.7462e+00,\n",
       "         -2.2568e-01,  3.4156e-01,  1.1291e+00, -5.4149e-01,  8.9995e-02],\n",
       "        [ 1.9516e-01, -9.4873e-01, -1.0013e+00,  8.1927e-01,  1.1754e+00,\n",
       "         -9.5265e-01,  3.9494e-01,  1.0641e+00, -7.4367e-01, -9.1595e-01],\n",
       "        [ 6.7366e-01,  4.3697e-01, -1.0321e+00,  1.0184e+00,  7.9057e-01,\n",
       "         -2.2012e+00, -1.5062e+00, -9.2036e-01,  7.9782e-01, -6.8590e-01],\n",
       "        [ 1.2667e+00,  5.1547e-01,  4.0250e-02,  4.7332e-01, -3.4081e+00,\n",
       "          9.7934e-01, -6.2778e-01, -6.4345e-01, -4.2259e-01, -2.0389e+00],\n",
       "        [ 1.3120e+00, -6.5225e-01, -7.2327e-02,  6.9240e-01, -3.0713e-01,\n",
       "          1.5127e-01, -2.9828e-01, -1.3058e-01,  1.5415e+00,  9.2312e-01],\n",
       "        [ 3.5685e-01, -5.5825e-01, -9.6742e-01,  2.8807e-01, -6.5753e-01,\n",
       "         -5.5028e-01,  1.1724e+00, -4.0424e-01,  9.9825e-01,  1.5521e+00],\n",
       "        [-3.8466e-01, -2.4367e+00, -1.2824e+00, -4.8190e-02, -1.2691e+00,\n",
       "         -1.0622e+00, -3.7848e-01,  3.5712e-01, -2.1998e-01,  6.9842e-01],\n",
       "        [ 4.3094e-01,  2.2849e+00, -6.1397e-02, -1.3735e+00, -8.3826e-01,\n",
       "          2.8082e-01, -1.6214e+00, -9.8592e-02,  2.3820e-01,  6.4539e-01],\n",
       "        [ 5.1120e-03,  2.6412e+00, -6.0560e-02,  2.6346e-01, -3.7448e-01,\n",
       "         -1.2713e+00,  1.9445e+00, -1.8786e+00,  4.7681e-01,  9.9898e-01],\n",
       "        [-1.1024e-01, -1.7503e+00, -4.2080e-01,  9.7669e-01,  1.8653e+00,\n",
       "         -1.0964e+00, -7.2280e-01, -5.7620e-02,  1.0736e+00, -5.5172e-01],\n",
       "        [-3.2976e+00,  4.9269e-01, -2.1470e+00, -1.5373e+00,  2.5667e-01,\n",
       "         -5.0945e-01,  1.3833e+00,  4.4387e-01,  1.2159e+00, -3.5927e-01],\n",
       "        [-3.8018e-01,  5.5394e-01, -8.1393e-01,  3.0322e-01, -2.0470e+00,\n",
       "         -1.1142e+00,  1.6251e-01, -6.0696e-01, -8.9233e-01,  6.1614e-01],\n",
       "        [-2.6294e+00,  2.2688e+00, -1.1149e+00, -1.9014e+00,  7.2222e-01,\n",
       "          6.8897e-01, -4.3300e-01, -3.8781e-01,  8.6987e-01, -1.4475e-03],\n",
       "        [ 7.9893e-01, -3.2958e-01, -2.2036e+00,  2.7113e-01, -2.0318e-01,\n",
       "         -2.5687e+00, -1.3300e+00,  1.6570e+00,  1.2948e+00, -8.2290e-01],\n",
       "        [ 1.6969e-01, -3.2497e-01,  1.3247e-01, -5.5226e-01,  7.1044e-01,\n",
       "          4.0420e-01,  1.8992e-01,  3.4159e-01, -4.4539e-01,  2.8329e-01],\n",
       "        [ 2.9689e-01, -5.7661e-01,  7.2083e-01, -5.5266e-01, -7.9555e-01,\n",
       "          2.9317e-02, -1.5621e-01,  5.3420e-01, -2.0438e-01, -1.2732e+00],\n",
       "        [-4.1898e-02,  1.9712e-01, -1.2699e+00, -6.0313e-01,  4.6275e-01,\n",
       "          3.1828e-01, -7.1777e-01, -2.7644e-01,  1.0717e-01,  5.0267e-01],\n",
       "        [-1.0116e+00, -6.7913e-01,  3.0188e-01,  1.1308e+00, -3.7244e-03,\n",
       "         -2.3568e-01,  4.5939e-01, -1.2322e-01,  3.8576e-01,  2.2713e+00],\n",
       "        [-1.5857e+00,  9.0337e-01,  1.0532e+00,  4.3823e-01,  1.7954e+00,\n",
       "         -1.1830e+00,  4.6693e-01, -5.0445e-01,  6.7062e-01, -3.8993e-01],\n",
       "        [ 3.9873e-01,  3.4349e-01, -3.6671e-01, -2.3369e-01, -5.4117e-01,\n",
       "          2.5991e-01, -5.0591e-01,  1.7485e+00,  1.1660e+00,  7.4267e-01],\n",
       "        [ 1.8748e+00,  4.4544e-01,  4.8673e-01,  1.2596e+00,  1.2160e+00,\n",
       "          8.1911e-01, -3.7606e-01, -5.4393e-01, -1.2823e+00, -8.8831e-01],\n",
       "        [-7.8845e-01, -5.3795e-01,  8.1033e-01,  8.6263e-01,  4.9495e-01,\n",
       "          4.4472e-01, -4.8289e-01,  7.5505e-01, -1.8197e+00,  9.2027e-01],\n",
       "        [-2.9631e-01,  1.0717e+00, -8.9897e-01,  1.4232e+00, -1.9254e-02,\n",
       "         -3.1979e-01, -5.4509e-01,  1.8381e+00, -2.0145e+00, -8.9181e-01],\n",
       "        [ 2.2816e-01, -1.9771e+00, -1.4226e+00, -4.7567e-01,  4.8518e-01,\n",
       "         -2.1026e-01,  1.3992e-01,  9.3136e-01,  2.8655e-01,  3.0421e+00],\n",
       "        [ 4.9298e-01, -2.9926e-01, -3.0391e+00, -1.2554e+00,  1.3700e+00,\n",
       "          2.7583e-01, -1.1260e+00, -6.1795e-01,  1.8448e+00, -1.0865e+00],\n",
       "        [-4.1379e-01,  9.8246e-01, -8.7099e-01, -6.0074e-01,  9.2290e-01,\n",
       "         -1.7212e+00,  6.7859e-01,  1.6090e+00, -1.8256e+00,  8.7454e-01],\n",
       "        [-4.2957e-01,  1.3094e+00, -1.0221e+00, -8.6803e-01, -1.3658e+00,\n",
       "         -1.7960e-01,  1.5464e+00, -1.1656e+00, -1.0281e+00, -1.1384e+00],\n",
       "        [ 2.0989e+00,  2.9132e-01, -1.1711e+00, -4.4960e-01,  5.7378e-01,\n",
       "          8.3089e-01,  1.2560e-02,  2.1994e+00,  3.7373e-01,  8.3932e-01],\n",
       "        [ 3.1935e-01,  6.8925e-01, -6.1438e-01, -5.5835e-01,  1.5917e+00,\n",
       "         -3.6149e-02,  1.9710e-01,  9.5139e-01, -3.2635e-02,  9.0856e-01],\n",
       "        [-5.5482e-01, -8.8597e-01,  9.3789e-01,  1.6435e+00,  4.3104e-01,\n",
       "          2.5520e+00,  3.6533e-01, -5.3034e-02,  1.2878e-01, -1.2543e+00],\n",
       "        [ 6.6137e-02,  1.7244e+00, -2.1363e-01, -2.5337e-01,  1.9033e+00,\n",
       "         -7.9987e-01, -2.4549e+00, -2.5247e-01,  3.5221e-02,  1.7030e+00],\n",
       "        [-9.7935e-01,  1.1185e-01,  4.5376e-01, -5.2932e-01,  5.0243e-01,\n",
       "         -5.3114e-01, -7.2555e-01,  5.6641e-01, -1.1002e+00, -1.9108e-01],\n",
       "        [-3.5194e-01,  1.6758e+00, -4.8917e-02,  3.2769e-01, -2.7603e-01,\n",
       "          9.0739e-02, -1.6020e+00,  7.6305e-01,  1.9297e+00,  1.4405e+00],\n",
       "        [-5.7857e-01,  7.1432e-01,  1.9217e-01,  2.5333e-01, -3.1493e-01,\n",
       "          2.5527e-02, -1.5609e+00, -3.7494e-01,  7.1957e-01,  1.9295e+00],\n",
       "        [ 4.0089e-01,  9.1554e-01,  6.2293e-01, -1.8383e-01,  1.7557e-01,\n",
       "          8.0565e-01, -1.8388e+00, -3.0944e-01, -1.5341e+00, -2.0053e-01],\n",
       "        [-9.3207e-01,  3.0546e-01,  1.4964e+00,  2.7901e-01, -5.3532e-01,\n",
       "          1.1382e+00,  1.3459e+00, -4.6433e-01, -3.7430e-01,  9.7059e-01],\n",
       "        [-1.7437e+00, -6.9619e-01, -4.5671e-01,  3.5035e-01, -1.5301e+00,\n",
       "          9.9324e-01,  3.9281e-01,  3.4275e-02,  1.1328e-01,  6.0981e-01],\n",
       "        [-1.2917e+00, -6.4949e-01,  5.9661e-01,  1.6067e+00, -1.3930e+00,\n",
       "         -8.5107e-01, -1.2457e+00,  6.1285e-01, -2.3512e+00,  2.7874e+00],\n",
       "        [ 8.0121e-02, -4.8926e-01,  8.1951e-01,  1.1793e+00,  1.9530e-02,\n",
       "          1.1249e+00, -9.6057e-01,  1.6481e+00, -3.5211e-01,  2.9334e-01],\n",
       "        [-1.3916e+00, -9.6024e-01, -3.4467e-01,  1.3889e+00,  1.8720e+00,\n",
       "         -1.2971e+00, -1.4842e+00, -1.4814e+00,  2.7518e-01, -7.9594e-01],\n",
       "        [-4.4727e-01, -2.1227e-01,  1.5573e+00, -4.0972e-01,  1.4496e+00,\n",
       "         -1.1523e+00,  7.8939e-01, -6.8262e-01,  2.5985e-01, -6.5789e-01],\n",
       "        [ 1.3753e+00, -1.7804e-01, -2.3070e-02, -1.5234e+00, -2.7596e+00,\n",
       "         -8.7945e-01,  9.8539e-01,  2.7572e-01,  5.8398e-01,  4.7783e-01],\n",
       "        [ 2.9841e-01, -1.0175e+00,  1.0369e+00, -1.9965e-01,  8.8951e-01,\n",
       "          6.9933e-01,  2.6168e-01,  5.9566e-01,  1.3539e+00, -6.6576e-01],\n",
       "        [-2.1484e-01,  3.3935e-01, -9.4403e-01,  1.6776e+00,  1.0093e+00,\n",
       "          1.3992e+00, -1.8118e+00,  1.3753e+00,  3.1352e-03,  9.9277e-01],\n",
       "        [-8.2438e-01, -4.1930e-01, -2.3864e+00, -1.6398e-02, -3.0102e-01,\n",
       "         -1.0519e+00, -9.4282e-01,  2.6173e-01,  1.6897e-01,  7.5831e-01],\n",
       "        [-1.8888e-01, -7.0749e-01, -7.6072e-01,  2.4097e-01,  9.8595e-01,\n",
       "         -2.1538e+00, -2.3858e-01, -7.2910e-02, -1.3154e+00,  7.4829e-02],\n",
       "        [ 1.2965e+00, -1.2495e+00, -3.6271e-01,  1.0932e+00, -1.2422e+00,\n",
       "          8.1897e-01,  9.5505e-02, -5.7854e-01,  1.7796e+00,  1.7567e+00],\n",
       "        [-6.9046e-01, -1.2082e+00, -3.1712e-01,  7.7323e-02, -9.7807e-01,\n",
       "          7.9218e-01, -8.4597e-01,  1.4992e-01,  8.6982e-01, -2.1095e+00],\n",
       "        [-1.2790e+00,  3.3823e-01,  3.7713e-01, -1.0281e+00, -2.2943e+00,\n",
       "          8.8626e-02,  4.8130e-01,  8.6356e-02,  9.6650e-01, -6.9136e-01],\n",
       "        [ 4.1465e-02,  5.5704e-01,  2.0924e-01, -9.6106e-01,  1.2761e-01,\n",
       "          8.6889e-01, -3.2581e-01,  3.8322e-02, -3.9738e-01,  3.0591e-01],\n",
       "        [-4.5717e-01,  4.5517e-01,  7.3681e-01, -1.1996e+00,  2.4085e-01,\n",
       "         -5.1856e-01,  5.9176e-01,  1.8744e-01,  1.0558e+00, -1.6138e+00],\n",
       "        [ 6.9332e-02,  1.0918e+00, -7.2829e-01,  7.0075e-01, -6.7003e-01,\n",
       "          5.5185e-01,  8.8332e-01, -9.1341e-01,  1.3710e-01,  6.5064e-01],\n",
       "        [ 1.1573e+00,  1.0756e+00, -9.4886e-01, -7.3652e-01,  2.3183e-01,\n",
       "         -1.2873e-01, -6.4604e-01,  3.4986e-01, -9.7541e-02,  1.3571e+00],\n",
       "        [-1.2762e+00, -2.0682e+00, -2.1079e+00, -6.4006e-01,  1.6063e+00,\n",
       "          1.9672e+00,  1.1706e+00, -8.1895e-01,  1.1935e+00, -2.2689e-01],\n",
       "        [ 2.7899e-01,  3.9660e-01, -5.5537e-01, -7.1233e-02,  3.8764e-01,\n",
       "          4.7112e-01, -7.7360e-01,  8.3587e-01,  7.0059e-02, -1.1442e+00],\n",
       "        [ 8.0487e-01, -6.1957e-01, -2.0732e+00, -1.7052e-01, -7.4558e-01,\n",
       "         -7.8979e-01, -2.2624e+00,  1.3371e+00, -5.5848e-01,  3.9303e-01],\n",
       "        [-5.2414e-01,  1.2648e+00, -2.6066e-01,  1.7807e+00, -1.2390e+00,\n",
       "          2.9718e-01, -8.6003e-01, -8.7327e-01, -1.2424e+00, -8.3714e-01],\n",
       "        [-1.3600e-02,  1.0433e+00,  6.3854e-01,  8.0580e-01,  4.1508e-01,\n",
       "          2.9487e-01,  1.3913e+00, -1.0389e+00,  7.1708e-01, -1.5650e+00],\n",
       "        [ 9.9385e-01, -1.8584e+00, -6.6693e-01, -1.5732e+00, -1.8678e-02,\n",
       "          1.2985e-01,  9.6171e-01, -2.2074e-01,  9.2940e-01,  8.7413e-01],\n",
       "        [-1.4030e+00, -1.5140e-01, -1.1806e+00,  2.7920e+00,  4.6707e-01,\n",
       "          1.0272e+00, -8.9967e-02, -1.2212e+00, -1.7997e+00,  7.0397e-01],\n",
       "        [ 3.5629e-01,  9.3955e-01,  9.2889e-02,  1.9935e-01, -2.6268e-01,\n",
       "         -1.3674e+00, -1.2293e+00,  9.3717e-01, -1.3611e+00, -5.6410e-01],\n",
       "        [-2.5102e-01,  4.9396e-01,  2.7984e+00,  1.6459e+00, -7.0938e-01,\n",
       "         -4.6671e-01, -1.5274e+00,  1.9340e-01,  1.9695e-01,  1.5840e-01],\n",
       "        [-1.2984e+00, -1.3124e-01,  1.0773e+00, -2.3709e+00,  1.1916e+00,\n",
       "          1.7998e-01,  5.2951e-01,  2.9063e+00,  1.4360e-01, -3.7009e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['forty', 'When'], 'winters'),\n",
       " (['winters', 'forty'], 'shall'),\n",
       " (['shall', 'winters'], 'besiege'),\n",
       " (['besiege', 'shall'], 'thy'),\n",
       " (['thy', 'besiege'], 'brow,'),\n",
       " (['brow,', 'thy'], 'And'),\n",
       " (['And', 'brow,'], 'dig'),\n",
       " (['dig', 'And'], 'deep'),\n",
       " (['deep', 'dig'], 'trenches'),\n",
       " (['trenches', 'deep'], 'in'),\n",
       " (['in', 'trenches'], 'thy'),\n",
       " (['thy', 'in'], \"beauty's\"),\n",
       " ([\"beauty's\", 'thy'], 'field,'),\n",
       " (['field,', \"beauty's\"], 'Thy'),\n",
       " (['Thy', 'field,'], \"youth's\"),\n",
       " ([\"youth's\", 'Thy'], 'proud'),\n",
       " (['proud', \"youth's\"], 'livery'),\n",
       " (['livery', 'proud'], 'so'),\n",
       " (['so', 'livery'], 'gazed'),\n",
       " (['gazed', 'so'], 'on'),\n",
       " (['on', 'gazed'], 'now,'),\n",
       " (['now,', 'on'], 'Will'),\n",
       " (['Will', 'now,'], 'be'),\n",
       " (['be', 'Will'], 'a'),\n",
       " (['a', 'be'], \"totter'd\"),\n",
       " ([\"totter'd\", 'a'], 'weed'),\n",
       " (['weed', \"totter'd\"], 'of'),\n",
       " (['of', 'weed'], 'small'),\n",
       " (['small', 'of'], 'worth'),\n",
       " (['worth', 'small'], 'held:'),\n",
       " (['held:', 'worth'], 'Then'),\n",
       " (['Then', 'held:'], 'being'),\n",
       " (['being', 'Then'], 'asked,'),\n",
       " (['asked,', 'being'], 'where'),\n",
       " (['where', 'asked,'], 'all'),\n",
       " (['all', 'where'], 'thy'),\n",
       " (['thy', 'all'], 'beauty'),\n",
       " (['beauty', 'thy'], 'lies,'),\n",
       " (['lies,', 'beauty'], 'Where'),\n",
       " (['Where', 'lies,'], 'all'),\n",
       " (['all', 'Where'], 'the'),\n",
       " (['the', 'all'], 'treasure'),\n",
       " (['treasure', 'the'], 'of'),\n",
       " (['of', 'treasure'], 'thy'),\n",
       " (['thy', 'of'], 'lusty'),\n",
       " (['lusty', 'thy'], 'days;'),\n",
       " (['days;', 'lusty'], 'To'),\n",
       " (['To', 'days;'], 'say,'),\n",
       " (['say,', 'To'], 'within'),\n",
       " (['within', 'say,'], 'thine'),\n",
       " (['thine', 'within'], 'own'),\n",
       " (['own', 'thine'], 'deep'),\n",
       " (['deep', 'own'], 'sunken'),\n",
       " (['sunken', 'deep'], 'eyes,'),\n",
       " (['eyes,', 'sunken'], 'Were'),\n",
       " (['Were', 'eyes,'], 'an'),\n",
       " (['an', 'Were'], 'all-eating'),\n",
       " (['all-eating', 'an'], 'shame,'),\n",
       " (['shame,', 'all-eating'], 'and'),\n",
       " (['and', 'shame,'], 'thriftless'),\n",
       " (['thriftless', 'and'], 'praise.'),\n",
       " (['praise.', 'thriftless'], 'How'),\n",
       " (['How', 'praise.'], 'much'),\n",
       " (['much', 'How'], 'more'),\n",
       " (['more', 'much'], 'praise'),\n",
       " (['praise', 'more'], \"deserv'd\"),\n",
       " ([\"deserv'd\", 'praise'], 'thy'),\n",
       " (['thy', \"deserv'd\"], \"beauty's\"),\n",
       " ([\"beauty's\", 'thy'], 'use,'),\n",
       " (['use,', \"beauty's\"], 'If'),\n",
       " (['If', 'use,'], 'thou'),\n",
       " (['thou', 'If'], 'couldst'),\n",
       " (['couldst', 'thou'], 'answer'),\n",
       " (['answer', 'couldst'], \"'This\"),\n",
       " ([\"'This\", 'answer'], 'fair'),\n",
       " (['fair', \"'This\"], 'child'),\n",
       " (['child', 'fair'], 'of'),\n",
       " (['of', 'child'], 'mine'),\n",
       " (['mine', 'of'], 'Shall'),\n",
       " (['Shall', 'mine'], 'sum'),\n",
       " (['sum', 'Shall'], 'my'),\n",
       " (['my', 'sum'], 'count,'),\n",
       " (['count,', 'my'], 'and'),\n",
       " (['and', 'count,'], 'make'),\n",
       " (['make', 'and'], 'my'),\n",
       " (['my', 'make'], 'old'),\n",
       " (['old', 'my'], \"excuse,'\"),\n",
       " ([\"excuse,'\", 'old'], 'Proving'),\n",
       " (['Proving', \"excuse,'\"], 'his'),\n",
       " (['his', 'Proving'], 'beauty'),\n",
       " (['beauty', 'his'], 'by'),\n",
       " (['by', 'beauty'], 'succession'),\n",
       " (['succession', 'by'], 'thine!'),\n",
       " (['thine!', 'succession'], 'This'),\n",
       " (['This', 'thine!'], 'were'),\n",
       " (['were', 'This'], 'to'),\n",
       " (['to', 'were'], 'be'),\n",
       " (['be', 'to'], 'new'),\n",
       " (['new', 'be'], 'made'),\n",
       " (['made', 'new'], 'when'),\n",
       " (['when', 'made'], 'thou'),\n",
       " (['thou', 'when'], 'art'),\n",
       " (['art', 'thou'], 'old,'),\n",
       " (['old,', 'art'], 'And'),\n",
       " (['And', 'old,'], 'see'),\n",
       " (['see', 'And'], 'thy'),\n",
       " (['thy', 'see'], 'blood'),\n",
       " (['blood', 'thy'], 'warm'),\n",
       " (['warm', 'blood'], 'when'),\n",
       " (['when', 'warm'], 'thou'),\n",
       " (['thou', 'when'], \"feel'st\"),\n",
       " ([\"feel'st\", 'thou'], 'it'),\n",
       " (['it', \"feel'st\"], 'cold.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62a3362af9fb780a0eb03402584da787fc1a1b0aa4d8f94c680e96f1c63d9193"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
