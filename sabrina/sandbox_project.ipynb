{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
    "# !gzip -d -k Video_Games_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497577, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('Video_Games_5.json', orient='records', lines=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fbc7bf8cd90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(df['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448431, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 17, 2015</td>\n",
       "      <td>A1HP7NVNPFMA4N</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>Ambrosia075</td>\n",
       "      <td>This game is a bit hard to get the hang of, bu...</td>\n",
       "      <td>but when you do it's great.</td>\n",
       "      <td>1445040000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>07 27, 2015</td>\n",
       "      <td>A1JGAP0185YJI6</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>travis</td>\n",
       "      <td>I played it a while but it was alright. The st...</td>\n",
       "      <td>But in spite of that it was fun, I liked it</td>\n",
       "      <td>1437955200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>02 20, 2015</td>\n",
       "      <td>A2204E1TH211HT</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>Grandma KR</td>\n",
       "      <td>found the game a bit too complicated, not what...</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1424390400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin reviewerName  \\\n",
       "0        5      True  10 17, 2015  A1HP7NVNPFMA4N  0700026657  Ambrosia075   \n",
       "1        4     False  07 27, 2015  A1JGAP0185YJI6  0700026657       travis   \n",
       "3        2      True  02 20, 2015  A2204E1TH211HT  0700026657   Grandma KR   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  This game is a bit hard to get the hang of, bu...   \n",
       "1  I played it a while but it was alright. The st...   \n",
       "3  found the game a bit too complicated, not what...   \n",
       "\n",
       "                                       summary  unixReviewTime vote style  \\\n",
       "0                  but when you do it's great.      1445040000  NaN   NaN   \n",
       "1  But in spite of that it was fun, I liked it      1437955200  NaN   NaN   \n",
       "3                                    Two Stars      1424390400  NaN   NaN   \n",
       "\n",
       "  image  \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "3   NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no3 = df[df['overall'].isin([1,2,4,5])]\n",
    "print(df_no3.shape)\n",
    "df_no3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_299/2452910105.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no3['sentiment'] = df_no3['overall'].map(dict_class)\n",
      "/tmp/ipykernel_299/2452910105.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no3['rev_sum'] = df_no3['summary'] + ' ' + df_no3['reviewText']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(448431, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_class = {\n",
    "    1 : 0,\n",
    "    2 : 0,\n",
    "    4 : 1,\n",
    "    5 : 1\n",
    "}\n",
    "\n",
    "# map reviews to sentiment classification\n",
    "df_no3['sentiment'] = df_no3['overall'].map(dict_class)\n",
    "df_no3['rev_sum'] = df_no3['summary'] + ' ' + df_no3['reviewText']\n",
    "# df_no3.head(3)\n",
    "\n",
    "# get only relevant columns\n",
    "df_games = pd.DataFrame()\n",
    "df_games = df_no3[['rev_sum', 'sentiment']]\n",
    "df_games.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_game, y_game = df_games['rev_sum'], df_games['sentiment']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_game, y_game, test_size=0.2, train_size = 0.8, stratify=y_game, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size = 0.5, train_size = 0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358744,), (44843,), (44844,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_dev.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "from https://dev.to/thepylot/compare-documents-similarity-using-python-nlp-4odp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X_train[0:100])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[:100].to_csv('X_train_100.txt', index=False)\n",
    "X_train_100 = list(X_train[:100])\n",
    "X_train_1000 = list(X_train[:1000])\n",
    "total_diff = 'My house is really small and this vacuum cleaner is perfect for it! Has enough power to pick up debris and clean carpets.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one big string\n",
    "df = pd.DataFrame(X_train_100)\n",
    "X_train_joint = ' '.join(df[0].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of strings, paragraph to sentences\n",
    "file_docs = sent_tokenize(X_train_joint)\n",
    "file2_docs = sent_tokenize(X_train_100[0])\n",
    "file3_docs = sent_tokenize(total_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_text(corpus1, corpus2):\n",
    "    \n",
    "    # tokenize corpus into sentences\n",
    "    # list of strings\n",
    "    first = sent_tokenize(corpus1)\n",
    "    second = sent_tokenize(corpus2)\n",
    "\n",
    "    # list of lists, words as tokens for corpus 1\n",
    "    gen_docs = [[w.lower() for w in word_tokenize(text)] for text in first]\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "    # # building the index\n",
    "    sims = gensim.similarities.Similarity('workdir/', tf_idf[corpus], num_features=len(dictionary))\n",
    "\n",
    "    for line in second:\n",
    "        # list of lists, words as tokens for corpus 2\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "\n",
    "        # update an existing dictionary and create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "    sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "    print(sum_of_sims)\n",
    "\n",
    "    print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "    \n",
    "    # percentage_of_similarity = round(float((sum_of_sims / len(file_docs))))\n",
    "    # print(percentage_of_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_text(X_train_100[0], X_train_100[3])\n",
    "compare_text(X_train_100[0], X_train_100[2])\n",
    "compare_text(X_train_100[0], X_train_100[1])\n",
    "compare_text(X_train_100[0], X_train_100[0])\n",
    "compare_text(X_train_100[0], total_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_text(X_train_joint, X_train_100[3])\n",
    "compare_text(X_train_joint, X_train_100[2])\n",
    "compare_text(X_train_joint, X_train_100[1])\n",
    "compare_text(X_train_joint, X_train_100[0])\n",
    "compare_text(X_train_joint, total_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim tut\n",
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in texts:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100 = list(X_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one big string\n",
    "df = pd.DataFrame(X_train_100)\n",
    "X_train_joint = ' '.join(df[0].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    Input: A string, can contain multiple sentences.\n",
    "    Output: A list of lists where each sublist is a sentence and the strings in it are the tokenised words.\n",
    "    '''\n",
    "    # list of strings\n",
    "    sentences = sent_tokenize(string)\n",
    "    # sentences = string.lower()\n",
    "\n",
    "    # list of lists, words as tokens for corpus 1\n",
    "    tokens = [[word.lower() for word in word_tokenize(text)] for text in sentences]\n",
    "    # tokens = [word_tokenize(sentences)]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(X_train_100[0])\n",
    "print(tokens)\n",
    "print('\\ntokens len = num. of sentences in string =', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(tokens)\n",
    "# dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "# corpus\n",
    "# (0, 1) means the word with id=0 appears once in the 1st sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # word frequency in corpus\n",
    "# for doc in corpus:\n",
    "#     print([[dictionary[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "# used tfid to weight down frequent words\n",
    "\n",
    "# # print if you want to see what it looks like\n",
    "# for doc in tf_idf[corpus]:\n",
    "#     print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class similarities.MatrixSimilarity is only appropriate when the whole set of vectors fits into memory. For example, a corpus of one million documents would require 2GB of RAM in a 256-dimensional LSI space, when used with this class.\n",
    "\n",
    "Without 2GB of free RAM, you would need to use the similarities.Similarity class. This class operates in fixed memory, by splitting the index across multiple files on disk, called shards. It uses similarities.MatrixSimilarity and similarities.SparseMatrixSimilarity internally, so it is still fast, although slightly more complex.\n",
    "\n",
    "From gensim docs: https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('workdir/', tf_idf[corpus], num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(tokens):\n",
    "    '''\n",
    "    Input: A list of lists where each sublist is a sentence and the strings in it are the tokenised words.\n",
    "    Output: returns indexed similarities\n",
    "    '''\n",
    "    dictionary = gensim.corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "    sims = gensim.similarities.Similarity('workdir/', tf_idf[corpus], num_features=len(dictionary))\n",
    "    print(sims)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to query using gensim: https://stackoverflow.com/questions/51287590/how-to-use-gensim-similarities-similarity-to-find-similarity-between-two-sentenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sim(string, sims):\n",
    "    query = [w.lower() for w in word_tokenize(string)] # tokenise into strings\n",
    "    # print(query)\n",
    "    query_bow = dictionary.doc2bow(query) # bag of words for string being queried\n",
    "    query_tf_idf = tf_idf[query_bow] # ??? \n",
    "    \n",
    "    # get similarity scores between given string and indexes text\n",
    "    sim_score =  sims[query_tf_idf]\n",
    "    # print(len(sim_score), sim_score)\n",
    "\n",
    "    # return the avarage sim between the queried string and all sentences in the indexed text - in the case review(s)\n",
    "    return sum(sim_score) / len(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sim('Nature is good for your mental health.'), query_sim('My house is really small and this vacuum cleaner is perfect for it! Has enough power to pick up debris and clean carpets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list = [] # similarities between each review and the full corpus\n",
    "for i in range(len(X_train_100)):\n",
    "    sim_list.append(round(query_sim(X_train_100[i]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(sim_list, kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize(string):\n",
    "    '''\n",
    "    Input: A string, can contain multiple sentences.\n",
    "    Output: A list of lists where each sublist is a sentence and the strings in it are the tokenised words.\n",
    "    '''\n",
    "    # list of strings\n",
    "    sentences = sent_tokenize(string)\n",
    "    # sentences = string.lower()\n",
    "\n",
    "    # list of lists, words as tokens for corpus 1\n",
    "    tokens = [[word.lower() for word in word_tokenize(text)] for text in sentences]\n",
    "    # tokens = [word_tokenize(sentences)]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def make_dict(tokens):\n",
    "    '''\n",
    "    Input: A list of lists where each sublist is a sentence and the strings in it are the tokenised words.\n",
    "    Output: returns indexed similarities\n",
    "    '''\n",
    "    dictionary = gensim.corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "    sims = gensim.similarities.Similarity('workdir/', tf_idf[corpus], num_features=len(dictionary))\n",
    "    print(sims)\n",
    "    return dictionary, tf_idf, sims\n",
    "\n",
    "\n",
    "def compare(string_index, string_query):\n",
    "    tokens = tokenize(string_index)\n",
    "    dictionary, tf_idf, sims = make_dict(tokens)\n",
    "\n",
    "    query = [w.lower() for w in word_tokenize(string_query)] # tokenise into strings\n",
    "    # print(query)\n",
    "    query_bow = dictionary.doc2bow(query) # bag of words for string being queried\n",
    "    query_tf_idf = tf_idf[query_bow] # ??? \n",
    "    \n",
    "    # get similarity scores between given string and indexes text\n",
    "    sim_score =  sims[query_tf_idf]\n",
    "    # print(len(sim_score), sim_score)\n",
    "\n",
    "    # return the avarage sim between the queried string and all sentences in the indexed text - in the case review(s)\n",
    "    return sum(sim_score) / len(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 1 documents in 0 shards (stored under workdir/)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_100 = list(X_train[:100])\n",
    "\n",
    "docs = [\n",
    "    'Nature is good for your mental health.', \n",
    "    'My house is really small and this vacuum cleaner is perfect for it! Has enough power to pick up debris and clean carpets.'\n",
    "]\n",
    "\n",
    "compare(docs[0], docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"hmmmm, what was I thinking I bought this for my PSP so that if I was traveling with the kids they could play it (as opposed to their leapster).  Well it is too difficult for them, and for me as an adult it is my last choice.  For a 8-12 year old, it is probably good, I just can't rate it high for myself or my kids.\",\n",
       " \"the best video game ever if you don't have this game\\nyou can stop call yourself a PC gamer\\nbecause you're not\\n\\nRainbow Six Vegas 2\\nit's not only a good game\\nbut is better of battlefield 3 and 4\\nespecially in co op mode\\nI buy Rainbow Six Vegas 2 yesterday and I play for 8hr 39mins\\n\\n*graphic 5 stars\\n*co op mode 5 stars\\n*gameplay 5 stars\\n*interface 5 stars\\n*sound 5 stars\\n*connectivity 5 stars\\n\\nyou can't go wrong with Rainbow Six Vegas 2\\noh PS rainbow six Vegas 2 run just fine with windows 7 64bit\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_100[0], X_train_100[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 3 documents in 0 shards (stored under workdir/)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27693048616250354"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(X_train_100[0], X_train_100[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('My house is really small and this vacuum cleaner is perfect for it! Has enough power to pick up debris and clean carpets.',\n",
       " \"the best video game ever if you don't have this game\\nyou can stop call yourself a PC gamer\\nbecause you're not\\n\\nRainbow Six Vegas 2\\nit's not only a good game\\nbut is better of battlefield 3 and 4\\nespecially in co op mode\\nI buy Rainbow Six Vegas 2 yesterday and I play for 8hr 39mins\\n\\n*graphic 5 stars\\n*co op mode 5 stars\\n*gameplay 5 stars\\n*interface 5 stars\\n*sound 5 stars\\n*connectivity 5 stars\\n\\nyou can't go wrong with Rainbow Six Vegas 2\\noh PS rainbow six Vegas 2 run just fine with windows 7 64bit\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1], X_train_100[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 2 documents in 0 shards (stored under workdir/)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.322748601436615"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(docs[1], X_train_100[1])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62a3362af9fb780a0eb03402584da787fc1a1b0aa4d8f94c680e96f1c63d9193"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
